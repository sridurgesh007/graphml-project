{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torch torch_geometric\n","!pip install deepchem\n","!pip install rdkit\n","!pip install optuna"],"metadata":{"id":"DjRdk4i9KVCH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download files\n","For all datasets and model files, download using this link: https://drive.google.com/drive/folders/1VFI8eS-SUUkvcUi4scdOVY5Ijq5J8boB?usp=sharing"],"metadata":{"id":"jHQoVHoASvKP"}},{"cell_type":"markdown","source":["# Data Pre-processing"],"metadata":{"id":"td8um_8KEh5c"}},{"cell_type":"markdown","source":["Fetching the data"],"metadata":{"id":"eyydKMO0EmgQ"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from rdkit import Chem\n","from rdkit.Chem import AllChem, MACCSkeys, Descriptors\n","import torch\n","from torch_geometric.data import Data\n","import deepchem as dc"],"metadata":{"id":"wZHC4RACEokJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set random seed for reproducibility\n","\n","np.random.seed(42)\n","torch.manual_seed(42)\n","os.makedirs(\"tox21_processed\", exist_ok=True)"],"metadata":{"id":"6QLd0ud6EtTy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"ðŸ“¥ Loading Tox21 dataset...\")\n","tasks, datasets, transformers = dc.molnet.load_tox21(featurizer=\"Raw\", data_dir=\".\", save_dir=\".\")\n","train_dataset, valid_dataset, test_dataset = datasets\n"],"metadata":{"id":"5wISpflkEvFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert datasets to pandas DataFrames for easier manipulation\n","train_df = train_dataset.to_dataframe()\n","valid_df = valid_dataset.to_dataframe()\n","test_df = test_dataset.to_dataframe()\n","\n","# map tasks to column names such as tasks[0] is y1 column, replace it with tasks[0]\n","for i, task in enumerate(tasks):\n","    train_df.rename(columns={f'y{i+1}': task}, inplace=True)\n","    valid_df.rename(columns={f'y{i+1}': task}, inplace=True)\n","    test_df.rename(columns={f'y{i+1}': task}, inplace=True)\n","\n","# save raw data to CSV files\n","train_df.to_csv(\"tox21_processed/train_raw.csv\", index=False)\n","valid_df.to_csv(\"tox21_processed/valid_raw.csv\", index=False)\n","test_df.to_csv(\"tox21_processed/test_raw.csv\", index=False)"],"metadata":{"id":"R1njt941ExNL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Basic EDA"],"metadata":{"id":"IkaJylEAE5j2"}},{"cell_type":"code","source":["# do some basic EDA\n","print(f\"Number of tasks: {len(tasks)}\")\n","print(f\"Tasks: {tasks}\")\n","print(f\"Training set size: {len(train_df)}\")\n","print(f\"Validation set size: {len(valid_df)}\")\n","print(f\"Test set size: {len(test_df)}\")\n","print(\"Sample data:\")\n","\n","print(\"âœ… Data loading and initial processing complete.\")\n","train_df.head()"],"metadata":{"id":"f9fzxXJuE91x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check for missing values\n","print(\"Missing values in training set:\")\n","print(train_df.isnull().sum())\n","print(\"Missing values in validation set:\")\n","print(valid_df.isnull().sum())\n","print(\"Missing values in test set:\")\n","print(test_df.isnull().sum())\n","\n","# distribution of labels for each task\n","for task in tasks:\n","    print(f\"Label distribution for task {task} in training set:\")\n","    print(train_df[task].value_counts(dropna=False))\n","    print(f\"Label distribution for task {task} in validation set:\")\n","    print(valid_df[task].value_counts(dropna=False))\n","    print(f\"Label distribution for task {task} in test set:\")\n","    print(test_df[task].value_counts(dropna=False))\n","\n","# visualize some molecules\n","print(\"Sample molecules from training set:\")\n","for smi in train_df['ids'].head(5):\n","    mol = Chem.MolFromSmiles(smi)\n","    display(Chem.Draw.MolToImage(mol))"],"metadata":{"id":"m3rY4L3AFB_q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extract molecular features for training basic machine learning models."],"metadata":{"id":"PNbIEZqIFluL"}},{"cell_type":"code","source":["# extract molecular features using RDKit and save to CSV files\n","def featurize_molecule(smi):\n","    mol = Chem.MolFromSmiles(smi)\n","    if mol is None:\n","        return None\n","    features = {}\n","    # Basic descriptors\n","    features['MolWt'] = Descriptors.MolWt(mol)\n","    features['NumHDonors'] = Descriptors.NumHDonors(mol)\n","    features['NumHAcceptors'] = Descriptors.NumHAcceptors(mol)\n","    features['TPSA'] = Descriptors.TPSA(mol)\n","    features['LogP'] = Descriptors.MolLogP(mol)\n","    # MACCS keys\n","    maccs = MACCSkeys.GenMACCSKeys(mol)\n","    for i in range(167):\n","        features[f'MACCS_{i}'] = int(maccs.GetBit(i))\n","    # Morgan fingerprint\n","    morgan_fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n","    for i in range(2048):\n","        features[f'Morgan_{i}'] = int(morgan_fp.GetBit(i))\n","    return features\n","\n","def featurize_dataset(df):\n","    feature_list = []\n","    for smi in tqdm(df['ids'], desc=\"Featurizing molecules\"):\n","        feats = featurize_molecule(smi)\n","        if feats is not None:\n","            feature_list.append(feats)\n","        else:\n","            feature_list.append({})  # Append empty dict for invalid SMILES\n","    features_df = pd.DataFrame(feature_list)\n","    return pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n","\n","train_features_df = featurize_dataset(train_df)\n","valid_features_df = featurize_dataset(valid_df)\n","test_features_df = featurize_dataset(test_df)\n","\n","# save featurized data to CSV files\n","train_features_df.to_csv(\"tox21_processed/train_featurized.csv\", index=False)\n","valid_features_df.to_csv(\"tox21_processed/valid_featurized.csv\", index=False)\n","test_features_df.to_csv(\"tox21_processed/test_featurized.csv\", index=False)\n"],"metadata":{"id":"7YrWQPCyGFpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check for any remaining missing values\n","print(\"Missing values in featurized training set:\")\n","print(train_features_df.isnull().sum().sum())\n","print(\"Missing values in featurized validation set:\")\n","print(valid_features_df.isnull().sum().sum())\n","print(\"Missing values in featurized test set:\")\n","print(test_features_df.isnull().sum().sum())"],"metadata":{"id":"0B442X7pJx8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize continuous features\n","from sklearn.preprocessing import StandardScaler\n","continuous_features = ['MolWt', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'LogP']\n","scaler = StandardScaler()\n","train_features_df[continuous_features] = scaler.fit_transform(train_features_df[continuous_features])\n","valid_features_df[continuous_features] = scaler.transform(valid_features_df[continuous_features])\n","test_features_df[continuous_features] = scaler.transform(test_features_df[continuous_features])\n","print(\"âœ… Normalization complete.\")"],"metadata":{"id":"ezQPBpHOJ0vE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save normalized data to CSV files\n","train_features_df.to_csv(\"tox21_processed/train_normalized.csv\", index=False)\n","valid_features_df.to_csv(\"tox21_processed/valid_normalized.csv\", index=False)\n","test_features_df.to_csv(\"tox21_processed/test_normalized.csv\", index=False)\n","print(\"âœ… Data processing pipeline complete. Processed files saved in 'tox21_processed' directory.\")"],"metadata":{"id":"NaFyFNDvJ49X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Models"],"metadata":{"id":"Dj547Xi1_K6U"}},{"cell_type":"markdown","source":["Random Forest"],"metadata":{"id":"sNc4DnMkKOsq"}},{"cell_type":"code","source":["# train a simple model to verify the pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","X_train = train_features_df[continuous_features + [col for col in train_features_df.columns if col.startswith('MACCS_') or col.startswith('Morgan_')]]\n","y_train = train_features_df[tasks].fillna(0).astype(int)\n","X_valid = valid_features_df[continuous_features + [col for col in valid_features_df.columns if col.startswith('MACCS_') or col.startswith('Morgan_')]]\n","y_valid = valid_features_df[tasks].fillna(0).astype(int)\n","X_test = test_features_df[continuous_features + [col for col in test_features_df.columns if col.startswith('MACCS_') or col.startswith('Morgan_')]]\n","y_test = test_features_df[tasks].fillna(0).astype(int)\n","model = RandomForestClassifier(n_estimators=100, random_state=42)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","print(\"Classification report on test set:\")\n","print(classification_report(y_test, y_pred, zero_division=0))\n","\n","# calculate roc_auc_score for each task\n","from sklearn.metrics import roc_auc_score\n","for i, task in enumerate(tasks):\n","    try:\n","        auc = roc_auc_score(y_test[task], y_xgb_pred[:, i])\n","        print(f\"ROC AUC for task {task}: {auc:.4f}\")\n","    except ValueError:\n","        print(f\"ROC AUC for task {task}: Cannot be computed (only one class present in y_true)\")\n","        continue\n","\n","print(\"âœ… Model training and evaluation complete.\")"],"metadata":{"id":"9dPLqqLrKQfP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DNN"],"metadata":{"id":"0jES9lgQCbOI"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, classification_report\n","\n","# ============================\n","# Improved Deep Neural Network\n","# ============================\n","class DeepToxDNN(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dims=[2048, 1024, 512, 256], dropout=0.5):\n","        super(DeepToxDNN, self).__init__()\n","\n","        layers = []\n","        prev_dim = input_dim\n","\n","        # Build hidden layers with batch norm and dropout\n","        for hidden_dim in hidden_dims:\n","            layers.extend([\n","                nn.Linear(prev_dim, hidden_dim),\n","                nn.BatchNorm1d(hidden_dim),\n","                nn.ReLU(),\n","                nn.Dropout(dropout)\n","            ])\n","            prev_dim = hidden_dim\n","\n","        # Output layer (no activation - use BCEWithLogitsLoss)\n","        layers.append(nn.Linear(prev_dim, output_dim))\n","\n","        self.network = nn.Sequential(*layers)\n","\n","        # Initialize weights\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","                if module.bias is not None:\n","                    nn.init.constant_(module.bias, 0)\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","# ============================\n","# Training Function\n","# ============================\n","def train_dnn(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    total_samples = 0\n","\n","    for X_batch, y_batch in train_loader:\n","        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(X_batch)\n","\n","        # Handle missing labels (if y contains NaN, mask them)\n","        mask = ~torch.isnan(y_batch)\n","        if mask.sum() == 0:\n","            continue\n","\n","        loss = criterion(logits[mask], y_batch[mask])\n","        loss.backward()\n","\n","        # Gradient clipping for stability\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()\n","\n","        total_loss += loss.item() * X_batch.size(0)\n","        total_samples += X_batch.size(0)\n","\n","    return total_loss / total_samples if total_samples > 0 else 0\n","\n","# ============================\n","# Evaluation Function\n","# ============================\n","def evaluate_dnn(model, loader, device, tasks):\n","    model.eval()\n","    all_probs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X_batch, y_batch in loader:\n","            X_batch = X_batch.to(device)\n","            logits = model(X_batch)\n","            probs = torch.sigmoid(logits)\n","\n","            all_probs.append(probs.cpu().numpy())\n","            all_labels.append(y_batch.cpu().numpy())\n","\n","    probs = np.vstack(all_probs)\n","    labels = np.vstack(all_labels)\n","\n","    # Compute per-task metrics\n","    aucs = []\n","    print(\"\\nPer-task ROC-AUC:\")\n","    for i, task in enumerate(tasks):\n","        # Filter valid labels (handle NaN if present)\n","        mask = ~np.isnan(labels[:, i])\n","        if mask.sum() < 10:\n","            print(f\"  {task}: Insufficient data\")\n","            continue\n","\n","        y_true = labels[mask, i]\n","        y_pred = probs[mask, i]\n","\n","        try:\n","            auc = roc_auc_score(y_true, y_pred)\n","            aucs.append(auc)\n","            print(f\"  {task}: {auc:.4f}\")\n","        except ValueError as e:\n","            print(f\"  {task}: Cannot compute - {e}\")\n","\n","    mean_auc = np.mean(aucs) if aucs else 0\n","    return mean_auc\n","\n","# ============================\n","# Main Training Script\n","# ============================\n","def main():\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\\n\")\n","\n","    # Hyperparameters\n","    input_dim = X_train.shape[1]\n","    output_dim = len(tasks)\n","    batch_size = 128\n","    learning_rate = 0.001\n","    num_epochs = 100\n","    patience = 15\n","\n","    print(f\"Input dimension: {input_dim}\")\n","    print(f\"Output dimension: {output_dim}\")\n","    print(f\"Training samples: {len(X_train)}\")\n","    print(f\"Validation samples: {len(X_valid)}\")\n","    print(f\"Test samples: {len(X_test)}\\n\")\n","\n","    # Convert to tensors (handle missing labels by keeping them as NaN)\n","    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n","    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n","\n","    X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32)\n","    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32)\n","\n","    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n","    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n","\n","    # Create data loaders\n","    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n","    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Initialize model\n","    model = DeepToxDNN(\n","        input_dim=input_dim,\n","        output_dim=output_dim,\n","        hidden_dims=[2048, 1024, 512, 256],\n","        dropout=0.5\n","    ).to(device)\n","\n","    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n","\n","    # Loss and optimizer\n","    criterion = nn.BCEWithLogitsLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n","    )\n","\n","    # Training loop\n","    best_val_auc = 0\n","    patience_counter = 0\n","\n","    print(\"=\"*60)\n","    print(\"Starting Training\")\n","    print(\"=\"*60)\n","\n","    for epoch in range(1, num_epochs + 1):\n","        train_loss = train_dnn(model, train_loader, optimizer, criterion, device)\n","\n","        if epoch % 5 == 0 or epoch == 1:\n","            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n","            print(f\"  Train Loss: {train_loss:.4f}\")\n","\n","            # Validation\n","            val_auc = evaluate_dnn(model, valid_loader, device, tasks)\n","            print(f\"  Mean Validation AUC: {val_auc:.4f}\")\n","\n","            # Learning rate scheduling\n","            scheduler.step(val_auc)\n","\n","            # Save best model\n","            if val_auc > best_val_auc:\n","                best_val_auc = val_auc\n","                torch.save(model.state_dict(), 'best_dnn_model.pt')\n","                patience_counter = 0\n","                print(f\"  âœ“ New best model saved! (AUC: {val_auc:.4f})\")\n","            else:\n","                patience_counter += 1\n","\n","            # Early stopping\n","            if patience_counter >= patience:\n","                print(f\"\\nEarly stopping at epoch {epoch}\")\n","                break\n","\n","    # Load best model and evaluate on test set\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Final Test Evaluation\")\n","    print(\"=\"*60)\n","\n","    model.load_state_dict(torch.load('best_dnn_model.pt'))\n","    test_auc = evaluate_dnn(model, test_loader, device, tasks)\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Mean Test ROC-AUC: {test_auc:.4f}\")\n","    print(f\"{'='*60}\")\n","\n","    # Detailed predictions for classification report\n","    model.eval()\n","    all_probs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X_batch, y_batch in test_loader:\n","            X_batch = X_batch.to(device)\n","            logits = model(X_batch)\n","            probs = torch.sigmoid(logits)\n","            all_probs.append(probs.cpu().numpy())\n","            all_labels.append(y_batch.cpu().numpy())\n","\n","    y_pred_probs = np.vstack(all_probs)\n","    y_true = np.vstack(all_labels)\n","    y_pred_binary = (y_pred_probs > 0.5).astype(int)\n","\n","    print(\"\\nClassification Report (averaged across tasks):\")\n","    # Handle NaN in labels for classification report\n","    valid_mask = ~np.isnan(y_true)\n","    if valid_mask.any():\n","        print(classification_report(\n","            y_true[valid_mask].astype(int).flatten(),\n","            y_pred_binary[valid_mask].flatten(),\n","            zero_division=0\n","        ))\n","\n","    print(\"\\nâœ… DNN training and evaluation complete.\")\n","\n","    return model, test_auc\n","\n","if __name__ == \"__main__\":\n","    model, test_auc = main()"],"metadata":{"id":"lTnSW5hmCXE0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GCN+GIN"],"metadata":{"id":"LFngJkLvCcrj"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv, GINConv, global_mean_pool, global_add_pool\n","from torch_geometric.loader import DataLoader\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, classification_report\n","\n","# ============================\n","# Improved GCN Model\n","# ============================\n","class ImprovedGCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n","        super(ImprovedGCN, self).__init__()\n","\n","        # 5 GCN layers with batch normalization\n","        self.conv1 = GCNConv(input_dim, hidden_dim)\n","        self.bn1 = nn.BatchNorm1d(hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n","        self.bn2 = nn.BatchNorm1d(hidden_dim)\n","        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n","        self.bn3 = nn.BatchNorm1d(hidden_dim)\n","        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n","        self.bn4 = nn.BatchNorm1d(hidden_dim)\n","        self.conv5 = GCNConv(hidden_dim, hidden_dim)\n","        self.bn5 = nn.BatchNorm1d(hidden_dim)\n","\n","        # 3-layer MLP head\n","        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n","        self.bn6 = nn.BatchNorm1d(hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n","        self.bn7 = nn.BatchNorm1d(hidden_dim // 2)\n","        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","\n","        # Graph convolutions with batch norm and ReLU\n","        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n","        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n","        x = F.relu(self.bn3(self.conv3(x, edge_index)))\n","        x = F.relu(self.bn4(self.conv4(x, edge_index)))\n","        x = F.relu(self.bn5(self.conv5(x, edge_index)))\n","\n","        # Global pooling\n","        x = global_add_pool(x, batch)\n","\n","        # MLP head with batch norm\n","        x = self.dropout(F.relu(self.bn6(self.fc1(x))))\n","        x = self.dropout(F.relu(self.bn7(self.fc2(x))))\n","        return self.fc3(x)  # Return logits for BCEWithLogitsLoss\n","\n","# ============================\n","# Improved GIN Model\n","# ============================\n","class ImprovedGIN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n","        super(ImprovedGIN, self).__init__()\n","\n","        # Helper function for GIN MLPs\n","        def mlp(in_dim, out_dim):\n","            return nn.Sequential(\n","                nn.Linear(in_dim, out_dim),\n","                nn.ReLU(),\n","                nn.Linear(out_dim, out_dim)\n","            )\n","\n","        # 5 GIN convolutional layers\n","        self.conv1 = GINConv(mlp(input_dim, hidden_dim))\n","        self.bn1 = nn.BatchNorm1d(hidden_dim)\n","        self.conv2 = GINConv(mlp(hidden_dim, hidden_dim))\n","        self.bn2 = nn.BatchNorm1d(hidden_dim)\n","        self.conv3 = GINConv(mlp(hidden_dim, hidden_dim))\n","        self.bn3 = nn.BatchNorm1d(hidden_dim)\n","        self.conv4 = GINConv(mlp(hidden_dim, hidden_dim))\n","        self.bn4 = nn.BatchNorm1d(hidden_dim)\n","        self.conv5 = GINConv(mlp(hidden_dim, hidden_dim))\n","        self.bn5 = nn.BatchNorm1d(hidden_dim)\n","\n","        # 3-layer MLP head\n","        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n","        self.bn6 = nn.BatchNorm1d(hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n","        self.bn7 = nn.BatchNorm1d(hidden_dim // 2)\n","        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","\n","        # Graph convolutions\n","        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n","        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n","        x = F.relu(self.bn3(self.conv3(x, edge_index)))\n","        x = F.relu(self.bn4(self.conv4(x, edge_index)))\n","        x = F.relu(self.bn5(self.conv5(x, edge_index)))\n","\n","        # Global pooling\n","        x = global_add_pool(x, batch)\n","\n","        # MLP head\n","        x = self.dropout(F.relu(self.bn6(self.fc1(x))))\n","        x = self.dropout(F.relu(self.bn7(self.fc2(x))))\n","        return self.fc3(x)  # Return logits\n","\n","# ============================\n","# Training Function\n","# ============================\n","def train_gnn(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    total_samples = 0\n","\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","\n","        logits = model(data)\n","        y = data.y.float().view(logits.shape)\n","\n","        # Only compute loss on labeled samples\n","        mask = (y >= 0) & (y <= 1)  # Valid labels\n","        if mask.sum() == 0:\n","            continue\n","\n","        loss = criterion(logits[mask], y[mask])\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * data.num_graphs\n","        total_samples += data.num_graphs\n","\n","    return total_loss / total_samples if total_samples > 0 else 0\n","\n","# ============================\n","# Evaluation Function\n","# ============================\n","def evaluate_gnn(model, loader, device, tasks):\n","    model.eval()\n","    all_probs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            logits = model(data)\n","            probs = torch.sigmoid(logits)\n","\n","            all_probs.append(probs.cpu().numpy())\n","            all_labels.append(data.y.cpu().numpy())\n","\n","    probs = np.vstack(all_probs)\n","    labels = np.vstack(all_labels)\n","\n","    # Compute AUC per task\n","    aucs = []\n","    for i, task in enumerate(tasks):\n","        # Filter valid labels\n","        mask = (labels[:, i] >= 0) & (labels[:, i] <= 1)\n","        if mask.sum() < 10:  # Need at least 10 samples\n","            print(f\"{task}: Insufficient data\")\n","            continue\n","\n","        y_true = labels[mask, i]\n","        y_pred = probs[mask, i]\n","\n","        try:\n","            auc = roc_auc_score(y_true, y_pred)\n","            aucs.append(auc)\n","            print(f\"{task}: {auc:.4f}\")\n","        except ValueError as e:\n","            print(f\"{task}: Cannot compute AUC - {e}\")\n","\n","    mean_auc = np.mean(aucs) if aucs else 0\n","    return mean_auc\n","\n","# ============================\n","# Main Training Script\n","# ============================\n","def main():\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # Hyperparameters\n","    input_dim = train_graphs[0].x.shape[1]\n","    hidden_dim = 128\n","    output_dim = len(tasks)\n","    batch_size = 64\n","    learning_rate = 0.001\n","    num_epochs = 100\n","    patience = 15\n","\n","    # Data loaders\n","    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n","    valid_loader = DataLoader(valid_graphs, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n","\n","    # Train both models\n","    for model_name, ModelClass in [(\"GCN\", ImprovedGCN), (\"GIN\", ImprovedGIN)]:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Training {model_name} Model\")\n","        print(f\"{'='*60}\")\n","\n","        model = ModelClass(input_dim, hidden_dim, output_dim).to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.BCEWithLogitsLoss()\n","\n","        best_val_auc = 0\n","        patience_counter = 0\n","\n","        for epoch in range(1, num_epochs + 1):\n","            train_loss = train_gnn(model, train_loader, optimizer, criterion, device)\n","\n","            if epoch % 5 == 0:\n","                print(f\"\\nEpoch {epoch}/{num_epochs} | Loss: {train_loss:.4f}\")\n","                print(\"Validation AUCs:\")\n","                val_auc = evaluate_gnn(model, valid_loader, device, tasks)\n","                print(f\"Mean Validation AUC: {val_auc:.4f}\")\n","\n","                if val_auc > best_val_auc:\n","                    best_val_auc = val_auc\n","                    torch.save(model.state_dict(), f'best_{model_name.lower()}_model.pt')\n","                    patience_counter = 0\n","                    print(f\"âœ“ New best model saved!\")\n","                else:\n","                    patience_counter += 1\n","\n","                if patience_counter >= patience:\n","                    print(f\"Early stopping at epoch {epoch}\")\n","                    break\n","\n","        # Load best model and evaluate on test set\n","        model.load_state_dict(torch.load(f'best_{model_name.lower()}_model.pt'))\n","        print(f\"\\n{'='*60}\")\n","        print(f\"{model_name} Test Results:\")\n","        print(f\"{'='*60}\")\n","        test_auc = evaluate_gnn(model, test_loader, device, tasks)\n","        print(f\"\\nMean Test AUC: {test_auc:.4f}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"NOvk8NA0Caru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Early Fusion Model"],"metadata":{"id":"5mcZrBkmERbt"}},{"cell_type":"markdown","source":["Build fusion dataset"],"metadata":{"id":"52jLmJEs_lNw"}},{"cell_type":"code","source":["\"\"\"\n","Builds the \"Early Fusion\" dataset from the raw DeepChem CSVs.\n","\n","This script performs all preprocessing:\n","1.  Loads the raw CSV data (train, valid, test).\n","2.  Generates three feature sets:\n","    - Graph Features (Nodes, Edges)\n","    - Fingerprint Features (ECFP4)\n","    - Descriptor Features (RDKit 2D)\n","3.  Normalizes the Node features using a StandardScaler fit *only* on the training set.\n","4.  Handles and skips invalid SMILES.\n","5.  Saves the final, processed data lists (for PyTorch Geometric) to disk.\n","\"\"\"\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from rdkit import Chem\n","from rdkit.Chem import AllChem, Descriptors\n","from sklearn.preprocessing import StandardScaler\n","import joblib\n","import torch\n","from torch_geometric.data import Data\n","\n","# --- Configuration ---\n","RAW_DATA_DIR = \"dataset\"\n","PROCESSED_DATA_DIR = \"processed_fusion_data\"\n","SCALER_PATH = os.path.join(PROCESSED_DATA_DIR, \"node_feature_scaler.joblib\")\n","\n","# Column names from the CSV\n","SMILES_COLUMN = 'ids'\n","LABEL_COLUMNS = [\n","    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD',\n","    'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n","]\n","WEIGHT_COLUMNS = [\n","    'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10', 'w11', 'w12'\n","]\n","\n","# --- RDKit Descriptors ---\n","# List of 2D descriptors to calculate\n","DESCRIPTOR_FUNCTIONS = {\n","    name: func for name, func in Descriptors.descList\n","    if not any(prefix in name for prefix in ['Ipc', 'Kappa', 'Chi']) # Exclude some complex ones\n","    and '3D' not in name # Exclude 3D descriptors\n","}\n","# Sort to ensure consistent order\n","DESCRIPTOR_NAMES = sorted(DESCRIPTOR_FUNCTIONS.keys())\n","print(f\"Calculating {len(DESCRIPTOR_NAMES)} RDKit descriptors.\")\n","\n","\n","# --- 1. Graph Featurization Functions ---\n","\n","def get_atom_features(atom):\n","    \"\"\" Generates a feature vector for a single atom. \"\"\"\n","    # One-hot encodings for categorical features\n","    symbol = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'I', 'other']\n","    degree = [0, 1, 2, 3, 4, 5, 6]\n","    hybridization = [\n","        Chem.rdchem.HybridizationType.SP,\n","        Chem.rdchem.HybridizationType.SP2,\n","        Chem.rdchem.HybridizationType.SP3,\n","        Chem.rdchem.HybridizationType.SP3D,\n","        Chem.rdchem.HybridizationType.SP3D2,\n","        'other'\n","    ]\n","    formal_charge = [-2, -1, 0, 1, 2]\n","    num_hydrogens = [0, 1, 2, 3, 4]\n","\n","    # Get features\n","    atom_symbol = atom.GetSymbol()\n","    atom_degree = atom.GetDegree()\n","    atom_hybrid = atom.GetHybridization()\n","    atom_charge = atom.GetFormalCharge()\n","    atom_hs = atom.GetTotalNumHs()\n","\n","    features = []\n","    # Symbol\n","    features.extend([int(atom_symbol == s) for s in symbol[:-1]])\n","    if sum(features[-len(symbol)+1:]) == 0: features.append(1) # 'other'\n","    else: features.append(0)\n","    # Degree\n","    features.extend([int(atom_degree == d) for d in degree])\n","    # Hybridization\n","    features.extend([int(atom_hybrid == h) for h in hybridization[:-1]])\n","    if sum(features[-len(hybridization)+1:]) == 0: features.append(1) # 'other'\n","    else: features.append(0)\n","    # Formal Charge\n","    features.extend([int(atom_charge == c) for c in formal_charge])\n","    # Num Hydrogens\n","    features.extend([int(atom_hs == h) for h in num_hydrogens])\n","    # Boolean features\n","    features.append(int(atom.GetIsAromatic()))\n","    features.append(int(atom.IsInRing()))\n","\n","    # --- New features from your script (Bug fixed) ---\n","    features.append(atom.GetAtomicNum())\n","    features.append(atom.GetMass())\n","\n","    chirality = atom.GetChiralTag()\n","    features.append(int(chirality == Chem.rdchem.ChiralType.CHI_UNSPECIFIED))\n","    features.append(int(chirality == Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW))\n","    features.append(int(chirality == Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW))\n","\n","    return features\n","\n","def mol_to_graph_data_obj(mol):\n","    \"\"\" Converts an RDKit Mol object into a PyG Data object. \"\"\"\n","    if mol is None:\n","        return None\n","\n","    # Node features\n","    node_features = [get_atom_features(atom) for atom in mol.GetAtoms()]\n","    x = torch.tensor(node_features, dtype=torch.float)\n","\n","    # Edge index and edge features\n","    edge_indices = []\n","    edge_attrs = []\n","    for bond in mol.GetBonds():\n","        i = bond.GetBeginAtomIdx()\n","        j = bond.GetEndAtomIdx()\n","        edge_indices.extend([[i, j], [j, i]])\n","\n","        # Edge features\n","        bond_type = bond.GetBondType()\n","        bt_features = [\n","            int(bond_type == Chem.rdchem.BondType.SINGLE),\n","            int(bond_type == Chem.rdchem.BondType.DOUBLE),\n","            int(bond_type == Chem.rdchem.BondType.TRIPLE),\n","            int(bond_type == Chem.rdchem.BondType.AROMATIC),\n","        ]\n","        stereo = bond.GetStereo()\n","        stereo_feats = [\n","            int(stereo == Chem.rdchem.BondStereo.STEREONONE),\n","            int(stereo == Chem.rdchem.BondStereo.STEREOZ),\n","            int(stereo == Chem.rdchem.BondStereo.STEREOE),\n","            int(stereo == Chem.rdchem.BondStereo.STEREOANY),\n","        ]\n","\n","        attr = bt_features + [int(bond.GetIsConjugated()), int(bond.IsInRing())] + stereo_feats\n","        edge_attrs.extend([attr, attr]) # Add for both directions\n","\n","    if len(edge_indices) > 0:\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n","        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n","    else:\n","        edge_index = torch.empty((2, 0), dtype=torch.long)\n","        edge_attr = torch.empty((0, len(edge_attrs[0]) if edge_attrs else 10), dtype=torch.float) # Match dim\n","\n","    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n","\n","\n","# --- 2. Fingerprint Featurization Function ---\n","\n","def get_fingerprint_features(mol):\n","    \"\"\"\n","    Generates ECFP4 fingerprint.\n","    RDKit's 'MorganFingerprint' with radius=2 is equivalent to ECFP4.\n","    \"\"\"\n","    if mol is None:\n","        return None\n","    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n","    return fp.ToBitString()\n","\n","\n","# --- 3. Descriptor Featurization Function ---\n","\n","def get_descriptor_features(mol):\n","    \"\"\" Generates RDKit 2D descriptor features. \"\"\"\n","    if mol is None:\n","        return [np.nan] * len(DESCRIPTOR_NAMES)\n","\n","    # Calculate all descriptors\n","    mol_descriptors = {}\n","    for name, func in DESCRIPTOR_FUNCTIONS.items():\n","        try:\n","            mol_descriptors[name] = func(mol)\n","        except Exception:\n","            mol_descriptors[name] = np.nan # Handle calculation errors\n","\n","    # Return in consistent order\n","    return [mol_descriptors[name] for name in DESCRIPTOR_NAMES]\n","\n","\n","# --- Main Data Creation Function ---\n","\n","def create_data_list(df, scaler=None):\n","    \"\"\"\n","    Processes a DataFrame and creates a list of PyG Data objects,\n","    each populated with all three feature sets.\n","\n","    Args:\n","        df (pd.DataFrame): The raw data.\n","        scaler (StandardScaler, optional): A *fitted* scaler to apply to node features.\n","                                          If None, no scaling is done.\n","\n","    Returns:\n","        list: A list of processed torch_geometric.data.Data objects.\n","    \"\"\"\n","    data_list = []\n","\n","    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Featurizing molecules\"):\n","        smi = row[SMILES_COLUMN]\n","        mol = Chem.MolFromSmiles(smi)\n","\n","        # Add explicit H's\n","        if mol is not None:\n","            mol = Chem.AddHs(mol)\n","\n","        # 1. Graph Features\n","        graph_data = mol_to_graph_data_obj(mol)\n","        if graph_data is None:\n","            print(f\"Warning: Skipping invalid SMILES: {smi}\")\n","            continue\n","        # Apply normalization if scaler is provided\n","        if scaler is not None:\n","            try:\n","                graph_data.x = torch.tensor(scaler.transform(graph_data.x.numpy()), dtype=torch.float)\n","            except Exception as e:\n","                print(f\"Warning: Failed to scale features for {smi}. Skipping. Error: {e}\")\n","                continue\n","        # --- END FIX ---\n","\n","        # 2. Fingerprint Features\n","        fp_str = get_fingerprint_features(mol)\n","        if fp_str is None:\n","            print(f\"Warning: Skipping molecule with failed FP: {smi}\")\n","            continue\n","        fp_features = [int(b) for b in fp_str]\n","        graph_data.fp_features = torch.tensor(fp_features, dtype=torch.float).reshape(1, -1)\n","\n","        # 3. Descriptor Features\n","        desc_features = get_descriptor_features(mol)\n","        graph_data.desc_features = torch.tensor(desc_features, dtype=torch.float).reshape(1, -1)\n","\n","        # 4. Get Labels\n","        labels = list(row[LABEL_COLUMNS])\n","        graph_data.y = torch.tensor(labels, dtype=torch.float).reshape(1, 12)\n","\n","        # 5. Get Weights (Fix for Bug 1)\n","        weights = list(row[WEIGHT_COLUMNS])\n","        graph_data.w = torch.tensor(weights, dtype=torch.float).reshape(1, 12)\n","\n","        data_list.append(graph_data)\n","\n","    return data_list\n"],"metadata":{"id":"MQfJqbZI_dKR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model architecture"],"metadata":{"id":"hzwRYrl-_3sR"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import (\n","    GINEConv,\n","    global_mean_pool,\n","    global_max_pool\n",")\n","\n","class EarlyFusionModel(nn.Module):\n","    def __init__(self,\n","                 node_feature_dim,\n","                 edge_feature_dim,\n","                 fp_feature_dim,\n","                 desc_feature_dim,\n","                 n_tasks,\n","                 graph_hidden_dim=128,\n","                 graph_out_dim=64,\n","                 gnn_dropout=0.2,\n","                 fp_out_dim=256,\n","                 classifier_dropout_1=0.5,\n","                 classifier_dropout_2=0.3\n","                ):\n","\n","        super(EarlyFusionModel, self).__init__()\n","\n","        # --- 1. GNN Branch (Using GINEConv) ---\n","        # ... (GNN branch code is unchanged) ...\n","        nn1 = nn.Sequential(\n","            nn.Linear(node_feature_dim, graph_hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(graph_hidden_dim, graph_hidden_dim)\n","        )\n","        self.gnn_conv1 = GINEConv(nn1, edge_dim=edge_feature_dim)\n","        nn2 = nn.Sequential(\n","            nn.Linear(graph_hidden_dim, graph_hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(graph_hidden_dim, graph_hidden_dim)\n","        )\n","        self.gnn_conv2 = GINEConv(nn2, edge_dim=edge_feature_dim)\n","        self.gnn_batch_norm1 = nn.BatchNorm1d(graph_hidden_dim)\n","        self.gnn_batch_norm2 = nn.BatchNorm1d(graph_hidden_dim)\n","\n","        self.gnn_dropout = gnn_dropout\n","\n","        gnn_mlp_in_dim = graph_hidden_dim * 2\n","        self.graph_fc = nn.Sequential(\n","            nn.Linear(gnn_mlp_in_dim, graph_out_dim),\n","            nn.ReLU(),\n","            nn.Dropout(gnn_dropout)\n","        )\n","\n","        # --- 2. Fingerprint (FP) Branch ---\n","        # ... (FP branch code is unchanged) ...\n","        self.fp_fc = nn.Sequential(\n","            nn.Linear(fp_feature_dim, fp_out_dim),\n","            nn.ReLU(),\n","            nn.Dropout(classifier_dropout_1)\n","        )\n","\n","        # --- 3. Descriptor (Desc) Branch ---\n","        desc_output_size = desc_feature_dim\n","\n","        # --- 4. Learnable Branch Weights (NEW!) ---\n","        # We create a learnable parameter vector with 3 weights, one for each branch.\n","        # We initialize them all to 1.0.\n","        self.branch_weights = nn.Parameter(torch.ones(3))\n","\n","        # --- 5. Final Classifier Head (Fusion) ---\n","        classifier_in_dim = graph_out_dim + fp_out_dim + desc_output_size\n","\n","        # ... (Classifier code is unchanged) ...\n","        self.classifier = nn.Sequential(\n","            nn.Linear(classifier_in_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(classifier_dropout_2),\n","            nn.Linear(128, n_tasks)\n","        )\n","\n","    # ... (forward_graph helper function is unchanged) ...\n","    def forward_graph(self, x, edge_index, edge_attr, batch):\n","        # GNN Conv 1\n","        x = self.gnn_conv1(x, edge_index, edge_attr)\n","        x = self.gnn_batch_norm1(x)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=self.gnn_dropout, training=self.training)\n","        # GNN Conv 2\n","        x = self.gnn_conv2(x, edge_index, edge_attr)\n","        x = self.gnn_batch_norm2(x)\n","        x = F.elu(x)\n","        # Readout/Pooling\n","        mean_pool = global_mean_pool(x, batch)\n","        max_pool = global_max_pool(x, batch)\n","        # Concatenate the two pooling results\n","        graph_out = torch.cat([mean_pool, max_pool], dim=1)\n","        # Pass through GNN's MLP\n","        graph_out = self.graph_fc(graph_out)\n","        return graph_out\n","\n","    def forward(self, graph_data):\n","\n","        x, edge_index, edge_attr, fp_features, desc_features, batch = graph_data.x, graph_data.edge_index, graph_data.edge_attr, graph_data.fp_features, graph_data.desc_features, graph_data.batch\n","\n","        # 1. GNN Branch\n","        graph_out = self.forward_graph(x, edge_index, edge_attr, batch)\n","\n","        # 2. FP Branch\n","        fp_out = self.fp_fc(fp_features)\n","\n","        # 3. Descriptor Branch\n","        desc_out = desc_features\n","\n","        # 4. Fusion (NEW: Apply learned weights!)\n","        # We multiply each branch output by its learned weight before concatenating.\n","        # This allows the model to scale the \"importance\" of each branch.\n","        fused_vector = torch.cat([\n","            graph_out * self.branch_weights[0],\n","            fp_out * self.branch_weights[1],\n","            desc_out * self.branch_weights[2]\n","        ], dim=1)\n","\n","        # 5. Final Classification\n","        out = self.classifier(fused_vector)\n","        return out\n","\n","# =============================================================================\n","# --- SELF-TEST BLOCK ---\n","# To run this test: `python model.py`\n","# =============================================================================\n","# if __name__ == \"__main__\":\n","\n","#     print(\"--- Running Model Self-Test ---\")\n","\n","#     # --- 1. Define Mock Dimensions ---\n","#     B = 4  # Batch size\n","#     N_TASKS = 12\n","\n","#     # Feature dimensions (must match build_fusion_dataset.py)\n","#     NODE_DIM = 41\n","#     EDGE_DIM = 11\n","#     FP_DIM = 2048\n","#     DESC_DIM = 200\n","\n","#     # Model hyperparameters\n","#     GRAPH_HIDDEN = 128\n","#     GRAPH_OUT = 64\n","#     FP_OUT = 256\n","\n","#     # --- 2. Create a Dummy Batch (simulating PyG DataLoader) ---\n","#     # We'll create 4 \"molecules\" of different sizes\n","#     from torch_geometric.data import Data, Batch\n","#     # Mol 1: 3 nodes, 2 edges\n","#     d1 = Data(\n","#         x=torch.rand(3, NODE_DIM),\n","#         edge_index=torch.tensor([[0, 1], [1, 2]], dtype=torch.long).t().contiguous(),\n","#         edge_attr=torch.rand(2, EDGE_DIM),\n","#         fp=torch.rand(1, FP_DIM),\n","#         desc=torch.rand(1, DESC_DIM),\n","#         y=torch.rand(1, N_TASKS), # Not used in forward, but good to have\n","#         w=torch.rand(1, N_TASKS)  # Not used in forward\n","#     )\n","\n","#     # Mol 2: 5 nodes, 4 edges\n","#     d2 = Data(\n","#         x=torch.rand(5, NODE_DIM),\n","#         edge_index=torch.tensor([[0, 1, 1, 2], [1, 2, 3, 4]], dtype=torch.long).t().contiguous(),\n","#         edge_attr=torch.rand(4, EDGE_DIM),\n","#         fp=torch.rand(1, FP_DIM),\n","#         desc=torch.rand(1, DESC_DIM),\n","#         y=torch.rand(1, N_TASKS),\n","#         w=torch.rand(1, N_TASKS)\n","#     )\n","\n","#     # Mol 3: 2 nodes, 1 edge\n","#     d3 = Data(\n","#         x=torch.rand(2, NODE_DIM),\n","#         edge_index=torch.tensor([[0], [1]], dtype=torch.long).t().contiguous(),\n","#         edge_attr=torch.rand(1, EDGE_DIM),\n","#         fp=torch.rand(1, FP_DIM),\n","#         desc=torch.rand(1, DESC_DIM),\n","#         y=torch.rand(1, N_TASKS),\n","#         w=torch.rand(1, N_TASKS)\n","#     )\n","\n","#     # Mol 4: 4 nodes, 3 edges\n","#     d4 = Data(\n","#         x=torch.rand(4, NODE_DIM),\n","#         edge_index=torch.tensor([[0, 1, 2], [1, 2, 3]], dtype=torch.long).t().contiguous(),\n","#         edge_attr=torch.rand(3, EDGE_DIM),\n","#         fp=torch.rand(1, FP_DIM),\n","#         desc=torch.rand(1, DESC_DIM),\n","#         y=torch.rand(1, N_TASKS),\n","#         w=torch.rand(1, N_TASKS)\n","#     )\n","\n","#     # Create a PyG Batch from this list\n","#     data_list = [d1, d2, d3, d4]\n","#     data_batch = Batch.from_data_list(data_list)\n","\n","#     print(f\"Created a dummy batch of {B} graphs.\")\n","#     print(f\"  Batch.x shape (total nodes):         {data_batch.x.shape}\")\n","#     print(f\"  Batch.edge_index shape:              {data_batch.edge_index.shape}\")\n","#     print(f\"  Batch.edge_attr shape (total edges): {data_batch.edge_attr.shape}\")\n","#     print(f\"  Batch.fp_features shape:             {data_batch.fp_features.shape}\")\n","#     print(f\"  Batch.desc_features shape:           {data_batch.desc_features.shape}\")\n","#     print(f\"  Batch.batch vector shape:            {data_batch.batch.shape}\")\n","\n","#     # --- 3. Instantiate Model ---\n","#     model = EarlyFusionModel(\n","#         node_feature_dim=NODE_DIM,\n","#         edge_feature_dim=EDGE_DIM,\n","#         fp_feature_dim=FP_DIM,\n","#         desc_feature_dim=DESC_DIM,\n","#         n_tasks=N_TASKS,\n","#         graph_hidden_dim=GRAPH_HIDDEN,\n","#         graph_out_dim=GRAPH_OUT,\n","#         fp_out_dim=FP_OUT\n","#     )\n","\n","#     model.train() # Set to training mode\n","#     print(\"\\nModel instantiated successfully.\")\n","\n","#     # --- 4. Run Forward Pass ---\n","#     try:\n","#         out = model(data_batch)\n","\n","#         print(\"\\n--- TEST SUCCESSFUL ---\")\n","#         print(f\"Forward pass ran without errors.\")\n","#         print(f\"Input batch size:  {B}\")\n","#         print(f\"Output shape:      {out.shape}\")\n","\n","#         # Check if the output shape is correct\n","#         assert out.shape == (B, N_TASKS)\n","#         print(\"Output shape is correct! (Batch Size, Num Tasks)\")\n","\n","#     except Exception as e:\n","#         print(\"\\n--- TEST FAILED ---\")\n","#         print(f\"An error occurred during the forward pass:\")\n","#         print(e)\n","#         import traceback\n","#         traceback.print_exc()"],"metadata":{"id":"IsJ_0jy3_p6O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optuna tuning code (please uncomment the code to run)"],"metadata":{"id":"ukrZB3J2BVZa"}},{"cell_type":"code","source":["# import os\n","# import numpy as np\n","# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# from torch_geometric.loader import DataLoader\n","# from sklearn.metrics import roc_auc_score\n","# from tqdm import tqdm\n","# from torch.optim.lr_scheduler import ReduceLROnPlateau\n","# import optuna\n","\n","\n","# # Import our custom *tunable* model\n","# # from temp import EarlyFusionModel\n","# # import gine_branchw model\n","# from gine_with_branchw import EarlyFusionModel\n","\n","# # --- 1. Constants and Setup ---\n","# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# print(f\"Using device: {DEVICE}\")\n","\n","# DATA_DIR = \"processed_fusion_data\"\n","# # DB_STORAGE_PATH = \"sqlite:///tox21_tuning3.db\"\n","# # STUDY_NAME = \"early_fusion_v3\"\n","# DB_STORAGE_PATH = \"sqlite:///optuna_tuning_gine_bw_clsimb1.db\" # <-- New DB file\n","# STUDY_NAME = \"early_fusion_v5_gine\"      # <-- New study name\n","# LABEL_COLUMNS = [\n","#         'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER',\n","#         'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5',\n","#         'SR-HSE', 'SR-MMP', 'SR-p53'\n","#     ]\n","\n","# # Hyperparameters\n","# EPOCHS = 200      # Max epochs *per trial*\n","# PATIENCE = 15    # Early stopping patience\n","# N_TASKS = 12\n","\n","# # --- 2. Load Data (Load ONCE, outside the objective) ---\n","# print(\"Loading processed data...\")\n","# try:\n","#     train_data = torch.load(os.path.join(DATA_DIR, \"train_data.pt\"), weights_only=False)\n","#     valid_data = torch.load(os.path.join(DATA_DIR, \"valid_data.pt\"), weights_only=False)\n","# except FileNotFoundError:\n","#     print(f\"Error: Processed data not found in '{DATA_DIR}'.\")\n","#     print(\"Please run 'build_fusion_dataset.py' first.\")\n","#     exit()\n","\n","# # Get feature dimensions (ONCE)\n","# first_data = train_data[0]\n","# NODE_DIM = first_data.x.shape[1]\n","# EDGE_DIM = first_data.edge_attr.shape[1]\n","# FP_DIM = first_data.fp.shape[1]\n","# DESC_DIM = first_data.desc.shape[1]\n","# print(f\"Data loaded. Feature Dims: Node={NODE_DIM}, Edge={EDGE_DIM}, FP={FP_DIM}, Desc={DESC_DIM}\")\n","\n","# # --- Calculate pos_weight for Class Imbalance (NEW!) ---\n","# def calculate_pos_weights(data_list):\n","#     num_pos = torch.zeros(N_TASKS)\n","#     num_neg = torch.zeros(N_TASKS)\n","\n","#     for data in data_list:\n","#         labels = data.y.squeeze() # Shape [12]\n","#         weights = data.w.squeeze() # Shape [12]\n","#         is_valid = (weights > 0) & (~torch.isnan(labels))\n","\n","#         pos_mask = (labels == 1) & is_valid\n","#         neg_mask = (labels == 0) & is_valid\n","\n","#         # --- THIS IS THE FIX ---\n","#         # We add the boolean mask (shape [12]) directly.\n","#         # This correctly adds 1s and 0s element-wise.\n","#         # The old code had `.sum(dim=0)`, which was a bug.\n","#         num_pos += pos_mask\n","#         num_neg += neg_mask\n","#         # --- END OF FIX ---\n","\n","#     pos_weight = num_neg / (num_pos + 1e-6)\n","\n","#     # We clip the weights to be at most 15. This stops runaway gradients.\n","#     # pos_weight = torch.clamp(pos_weight, min=1.0, max=15.0)\n","\n","#     print(\"--- Class Imbalance (unCLIPPED pos_weight) Calculated ---\")\n","#     for name, weight in zip(LABEL_COLUMNS, pos_weight):\n","#         print(f\"  {name:<16}: {weight:.2f}\") # Now these will all be different!\n","#     print(\"-----------------------------------------------\")\n","\n","#     return pos_weight.to(DEVICE)\n","\n","# # Calculate weights *only* from the training set\n","# pos_weight = calculate_pos_weights(train_data)\n","\n","# # --- 3. Training/Evaluation Functions (copied from train.py) ---\n","\n","# # Loss function (defined globally)\n","# loss_fn = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","\n","# def train_epoch(model, loader, loss_fn, optimizer):\n","#     model.train()\n","#     total_loss = 0\n","#     for batch in loader:\n","#         batch = batch.to(DEVICE)\n","#         logits = model(batch)\n","#         y_true = batch.y\n","#         weights = batch.w\n","\n","#         raw_loss = loss_fn(logits, y_true)\n","#         weighted_loss = raw_loss * weights\n","#         final_loss = weighted_loss.sum() / (weights.sum() + 1e-8)\n","\n","#         optimizer.zero_grad()\n","#         final_loss.backward()\n","#         optimizer.step()\n","#         total_loss += final_loss.item() * batch.num_graphs\n","#     return total_loss / len(loader.dataset)\n","\n","# @torch.no_grad()\n","# def eval_model(model, loader):\n","#     model.eval()\n","#     all_preds, all_labels, all_weights = [], [], []\n","#     for batch in loader:\n","#         batch = batch.to(DEVICE)\n","#         logits = model(batch)\n","#         all_preds.append(torch.sigmoid(logits).cpu().numpy())\n","#         all_labels.append(batch.y.cpu().numpy())\n","#         all_weights.append(batch.w.cpu().numpy())\n","\n","#     all_preds = np.concatenate(all_preds, axis=0)\n","#     all_labels = np.concatenate(all_labels, axis=0)\n","#     all_weights = np.concatenate(all_weights, axis=0)\n","\n","#     task_aucs = []\n","#     for i in range(N_TASKS):\n","#         valid_indices = all_weights[:, i] > 0\n","#         if np.sum(valid_indices) > 0 and len(np.unique(all_labels[valid_indices, i])) == 2:\n","#             task_aucs.append(roc_auc_score(all_labels[valid_indices, i], all_preds[valid_indices, i]))\n","#         else:\n","#             task_aucs.append(np.nan)\n","\n","#     # print(f\"--- Trial {trial.number} AUCs ---\")\n","#     # use task names\n","#     for i, auc in enumerate(task_aucs):\n","#         print(f\"  Task {i+1} ({LABEL_COLUMNS[i]}): AUC = {auc:.4f}\" if not np.isnan(auc) else f\"  Task {i+1} ({LABEL_COLUMNS[i]}): AUC = N/A\")\n","\n","#     return np.nanmean(task_aucs)\n","\n","# # --- 4. Optuna Objective Function ---\n","\n","# def objective(trial):\n","#     \"\"\"\n","#     This function is called by Optuna for each trial.\n","#     \"\"\"\n","#     # --- A. Suggest Hyperparameters ---\n","#     print(f\"\\n--- Starting Trial {trial.number} ---\")\n","\n","#     # gnn type\n","#     # gnn = trial.suggest_categorical(\"gnn_type\", ['gat', 'gin'])\n","\n","#     # Optimization params\n","#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n","#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n","#     batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n","\n","#     # Model architecture params\n","#     # gat_heads = trial.suggest_categorical(\"gat_heads\", [2, 4, 8, 16])\n","#     gnn_dropout = trial.suggest_float(\"gnn_dropout\", 0.0, 0.5)\n","#     classifier_dropout_1 = trial.suggest_float(\"classifier_dropout_1\", 0.0, 0.6)\n","#     classifier_dropout_2 = trial.suggest_float(\"classifier_dropout_2\", 0.0, 0.5)\n","\n","#     # Fixed model params\n","#     graph_hidden_dim = trial.suggest_categorical(\"graph_hidden_dim\", [64, 128, 256])\n","#     graph_out_dim = trial.suggest_categorical(\"graph_out_dim\", [32, 64, 128])\n","#     fp_out_dim = trial.suggest_categorical(\"fp_out_dim\", [128, 256, 512])\n","\n","#     # --- B. Setup Model, Loaders, Optimizer ---\n","\n","#     # DataLoaders must be created inside objective to use batch_size\n","#     train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","#     valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n","\n","#     model = EarlyFusionModel(\n","#         node_feature_dim=NODE_DIM,\n","#         edge_feature_dim=EDGE_DIM,\n","#         fp_feature_dim=FP_DIM,\n","#         desc_feature_dim=DESC_DIM,\n","#         n_tasks=N_TASKS,\n","#         graph_hidden_dim=graph_hidden_dim,\n","#         graph_out_dim=graph_out_dim,\n","#         gnn_dropout=gnn_dropout,\n","#         fp_out_dim=fp_out_dim,\n","#         classifier_dropout_1=classifier_dropout_1,\n","#         classifier_dropout_2=classifier_dropout_2\n","#     ).to(DEVICE)\n","\n","#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","#     scheduler = ReduceLROnPlateau(\n","#         optimizer,\n","#         mode='max',      # We monitor AUC, so we want to maximize it\n","#         factor=0.5,      # Reduce LR by half\n","#         patience=5,      # Wait 5 epochs for improvement\n","#         min_lr=1e-7      # Don't go below this\n","#     )\n","\n","#     # --- C. Run Training Loop ---\n","\n","#     best_valid_auc = 0.0\n","#     epochs_no_improve = 0\n","\n","#     for epoch in range(1, EPOCHS + 1):\n","#         train_loss = train_epoch(model, train_loader, loss_fn, optimizer)\n","#         valid_auc = eval_model(model, valid_loader)\n","\n","#         scheduler.step(valid_auc)\n","\n","#         print(f\"Trial {trial.number} Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Valid AUC: {valid_auc:.4f}\")\n","\n","#         # Optuna Pruning: Stop unpromising trials early\n","#         trial.report(valid_auc, epoch)\n","#         if trial.should_prune():\n","#             print(\"--- Trial Pruned ---\")\n","#             raise optuna.TrialPruned()\n","\n","#         # Early Stopping\n","#         if valid_auc > best_valid_auc:\n","#             best_valid_auc = valid_auc\n","#             epochs_no_improve = 0\n","#         else:\n","#             epochs_no_improve += 1\n","\n","#         if epochs_no_improve >= PATIENCE:\n","#             print(f\"--- Trial Early-Stopped ---\")\n","#             break\n","\n","#     return best_valid_auc # Return the best validation AUC for this trial\n","\n","# # --- 5. Main Study Execution ---\n","\n","# if __name__ == \"__main__\":\n","#     print(f\"Starting Optuna study: {STUDY_NAME}\")\n","#     print(f\"Database will be saved to: {DB_STORAGE_PATH}\")\n","\n","#     # Create a study object and specify direction to \"maximize\" AUC\n","#     study = optuna.create_study(\n","#         study_name=STUDY_NAME,\n","#         storage=DB_STORAGE_PATH,\n","#         direction=\"maximize\",\n","#         load_if_exists=True  # Allows you to resume tuning\n","#     )\n","\n","#     # redoing trials with branchw\n","#     print(\"Running trials with branch weights\")\n","\n","#     # Start the optimization\n","#     try:\n","#         study.optimize(objective, n_trials=200)  # Run 200 trials\n","#     except KeyboardInterrupt:\n","#         print(\"Tuning interrupted by user.\")\n","\n","#     # --- 6. Print Results ---\n","#     print(\"\\n--- Tuning Complete ---\")\n","\n","#     pruned_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.PRUNED])\n","#     completed_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])\n","\n","#     print(\"Study statistics: \")\n","#     print(f\"  Number of finished trials: {len(study.trials)}\")\n","#     print(f\"  Number of pruned trials:   {len(pruned_trials)}\")\n","#     print(f\"  Number of complete trials: {len(completed_trials)}\")\n","\n","#     print(\"\\n--- Best Trial ---\")\n","#     trial = study.best_trial\n","#     print(f\"  Value (AUC): {trial.value:.4f}\")\n","#     print(\"  Params: \")\n","#     for key, value in trial.params.items():\n","#         print(f\"    {key}: {value}\")"],"metadata":{"id":"IpROlVcKA8ZP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training with branch and positive weights"],"metadata":{"id":"YJMYVr5mABeU"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch_geometric.loader import DataLoader\n","from sklearn.metrics import roc_auc_score\n","import numpy as np\n","import os\n","import joblib\n","from gine_with_branchw import EarlyFusionModel\n","\n","# --- Configuration ---\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","PROCESSED_DATA_DIR = \"processed_fusion_data_3d\"\n","N_TASKS = 12\n","MODEL_SAVE_PATH = \"test_run_best_model_class_imbalance_3d.pth\"\n","N_EPOCHS = 200\n","EARLY_STOP_PATIENCE = 15\n","TASK_NAMES = [\n","    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\", \"NR-ER\", \"NR-ER-LBD\",\n","    \"NR-PPAR-gamma\", \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n","]\n","DEFAULT_PARAMS = {\n","    'lr': 1e-4, 'weight_decay': 1e-5, 'batch_size': 64, 'gnn_dropout': 0.3,\n","    'classifier_dropout_1': 0.2, 'classifier_dropout_2': 0.5,\n","    'graph_hidden_dim': 128, 'graph_out_dim': 64, 'fp_out_dim': 256\n","}\n","\n","# ... --- Load Scalers (for model dimensions) ...\n","try:\n","    node_scaler = joblib.load(os.path.join(PROCESSED_DATA_DIR, \"node_feature_scaler.joblib\"))\n","    desc_imputer = joblib.load(os.path.join(PROCESSED_DATA_DIR, \"desc_feature_imputer.joblib\"))\n","    desc_scaler = joblib.load(os.path.join(PROCESSED_DATA_DIR, \"desc_feature_scaler.joblib\"))\n","except FileNotFoundError:\n","    print(\"Error: Scaler/imputer files not found. Please run 'build_fusion_dataset_3d.py' first.\")\n","    exit()\n","\n","# --- Determine Feature Dimensions ---\n","try:\n","    _temp_data_list = torch.load(os.path.join(PROCESSED_DATA_DIR, \"train_data.pt\"), weights_only=False)\n","    if not _temp_data_list: exit()\n","    _temp_data = _temp_data_list[0]\n","    NODE_FEATURE_DIM = _temp_data.x.shape[1]\n","    EDGE_FEATURE_DIM = _temp_data.edge_attr.shape[1]\n","    FP_FEATURE_DIM = _temp_data.fp_features.shape[1]\n","    DESC_FEATURE_DIM = _temp_data.desc_features.shape[1]\n","    print(f\"--- Feature Dimensions Detected ---\")\n","    print(f\"Node Features:   {NODE_FEATURE_DIM}\")\n","    print(f\"Edge Features:   {EDGE_FEATURE_DIM}\")\n","    print(f\"FP Features:     {FP_FEATURE_DIM}\")\n","    print(f\"Desc Features:   {DESC_FEATURE_DIM}\")\n","    print(f\"---------------------------------\")\n","    train_data_list = _temp_data_list\n","    valid_data_list = torch.load(os.path.join(PROCESSED_DATA_DIR, \"valid_data.pt\"), weights_only=False)\n","    test_data_list = torch.load(os.path.join(PROCESSED_DATA_DIR, \"test_data.pt\"), weights_only=False)\n","    del _temp_data, _temp_data_list\n","except Exception as e:\n","    print(f\"Error loading data: {e}\")\n","    exit()\n","except Exception as e:\n","    print(f\"Error loading data for dimension check: {e}\")\n","    exit()\n","\n","\n","# --- 1. Calculate pos_weight for Class Imbalance (NEW!) ---\n","def calculate_pos_weights(data_list):\n","    num_pos = torch.zeros(N_TASKS)\n","    num_neg = torch.zeros(N_TASKS)\n","\n","    for data in data_list:\n","        labels = data.y.squeeze() # Shape [12]\n","        weights = data.w.squeeze() # Shape [12]\n","        is_valid = (weights > 0) & (~torch.isnan(labels))\n","\n","        pos_mask = (labels == 1) & is_valid\n","        neg_mask = (labels == 0) & is_valid\n","\n","        # We add the boolean mask (shape [12]) directly.\n","        # This correctly adds 1s and 0s element-wise.\n","        num_pos += pos_mask\n","        num_neg += neg_mask\n","\n","    pos_weight = num_neg / (num_pos + 1e-6)\n","\n","    # We clip the weights to be at most 15. This stops runaway gradients.\n","    # pos_weight = torch.clamp(pos_weight, min=1.0, max=15.0)\n","\n","    print(\"--- Class Imbalance (unCLIPPED pos_weight) Calculated ---\")\n","    for name, weight in zip(TASK_NAMES, pos_weight):\n","        print(f\"  {name:<16}: {weight:.2f}\") # Now these will all be different!\n","    print(\"-----------------------------------------------\")\n","\n","    return pos_weight.to(DEVICE)\n","\n","# Calculate weights *only* from the training set\n","pos_weight_tensor = calculate_pos_weights(train_data_list)\n","\n","\n","# --- 2. Update Loss Function to use pos_weight (NEW!) ---\n","def weighted_bce_loss(y_pred, y_true, weights, pos_weight):\n","    \"\"\"\n","    Our full, robust loss function.\n","    - `weights` handles MISSING labels (w=0).\n","    - `pos_weight` handles CLASS IMBALANCE (rare positives).\n","    \"\"\"\n","    # pos_weight is shape [12], we give it to BCEWithLogitsLoss\n","    loss_fn = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","\n","    raw_loss = loss_fn(y_pred, y_true)\n","\n","    # Mask out NaN labels\n","    is_valid = ~torch.isnan(y_true)\n","    raw_loss = torch.where(is_valid, raw_loss, torch.zeros_like(raw_loss))\n","\n","    # Apply the missing-label weights\n","    weighted_loss = raw_loss * weights\n","\n","    # Normalize by the sum of weights\n","    total_weight = weights.sum()\n","    final_loss = weighted_loss.sum() / (total_weight + 1e-8)\n","    return final_loss\n","\n","# ... (eval_model is unchanged) ...\n","@torch.no_grad()\n","def eval_model(model, loader, print_scores=False):\n","    model.eval()\n","    all_preds, all_labels, all_weights = [], [], []\n","    for data in loader:\n","        data = data.to(DEVICE)\n","        out = model(data)\n","        all_preds.append(out.cpu())\n","        all_labels.append(data.y.cpu())\n","        all_weights.append(data.w.cpu())\n","    all_preds = torch.cat(all_preds, dim=0)\n","    all_labels = torch.cat(all_labels, dim=0)\n","    all_weights = torch.cat(all_weights, dim=0)\n","\n","    val_loss = weighted_bce_loss(all_preds.to(DEVICE), all_labels.to(DEVICE), all_weights.to(DEVICE), pos_weight_tensor)\n","    # print(f\"Validation Loss: {val_loss.item():.4f}\")\n","    task_aucs = []\n","    valid_labels_mask = ~torch.isnan(all_labels) & (all_weights > 0)\n","    for i in range(N_TASKS):\n","        task_labels = all_labels[valid_labels_mask[:, i], i]\n","        task_preds = all_preds[valid_labels_mask[:, i], i]\n","        if len(task_labels) > 1 and len(torch.unique(task_labels)) > 1:\n","            try: task_aucs.append(roc_auc_score(task_labels.numpy(), torch.sigmoid(task_preds).numpy()))\n","            except ValueError: task_aucs.append(np.nan)\n","        else: task_aucs.append(np.nan)\n","    if print_scores:\n","        print(\"\\n--- Per-Task ROC-AUC Scores ---\")\n","        for name, auc in zip(TASK_NAMES, task_aucs):\n","            if not np.isnan(auc): print(f\"  {name:<16}: {auc:.4f}\")\n","            else: print(f\"  {name:<16}: N/A (not enough samples)\")\n","        print(\"---------------------------------\")\n","    mean_auc = np.nanmean(task_aucs)\n","    return val_loss, mean_auc\n","\n","# --- Main Training Function ---\n","def main():\n","    print(\"--- Starting Test Run (with Imbalance Fix) ---\")\n","    print(f\"Using device: {DEVICE}\")\n","\n","    # Data loader creation\n","    print(f\"Loading {len(train_data_list)} train, {len(valid_data_list)} valid, {len(test_data_list)} test samples.\")\n","    train_loader = DataLoader(train_data_list, batch_size=8, shuffle=True)\n","    valid_loader = DataLoader(valid_data_list, batch_size=8, shuffle=False)\n","    test_loader = DataLoader(test_data_list, batch_size=8, shuffle=False)\n","\n","    # 3. Initialize Model (uses branch_weights) ---\n","    # we use the best parameters from optuna tuning\n","    model = EarlyFusionModel(\n","        node_feature_dim=NODE_FEATURE_DIM,\n","        edge_feature_dim=EDGE_FEATURE_DIM,\n","        fp_feature_dim=FP_FEATURE_DIM,\n","        desc_feature_dim=DESC_FEATURE_DIM,\n","        n_tasks=N_TASKS,\n","        graph_hidden_dim=256,\n","        graph_out_dim=64,\n","        fp_out_dim=64,\n","        gnn_dropout=0.317,\n","        classifier_dropout_1=0.572,\n","        classifier_dropout_2=0.432\n","    ).to(DEVICE)\n","\n","    print(f\"Model Architecture:{model}\")\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.00024, weight_decay=3.948e-06)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7)\n","    print(f\"Optimizer:{optimizer}\")\n","    print(f\"Scheduler:{scheduler}\")\n","\n","    # 4. Training Loop (Pass pos_weight_tensor to loss)\n","    best_valid_auc = 0.0\n","    epochs_no_improve = 0\n","\n","    for epoch in range(N_EPOCHS):\n","        model.train()\n","        total_loss = 0\n","        for data in train_loader:\n","            data = data.to(DEVICE)\n","            optimizer.zero_grad()\n","\n","            out = model(data)\n","\n","            loss = weighted_bce_loss(out, data.y, data.w, pos_weight_tensor)\n","\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item() * data.num_graphs\n","\n","        avg_train_loss = total_loss / len(train_loader.dataset)\n","        val_loss, valid_auc = eval_model(model, valid_loader, print_scores=False)\n","        print(f\"Epoch {epoch+1:02d}/{N_EPOCHS:02d} | Train Loss: {avg_train_loss:.4f} | Valid Loss: {val_loss.item():.4f} | Valid AUC: {valid_auc:.4f} | LR: {optimizer.param_groups[0]['lr']:.1e}\")\n","\n","        # Print the learned branch weights each epoch!\n","        weights = model.branch_weights.data.cpu().numpy()\n","        print(f\"  Branch Weights (GNN, FP, Desc): {weights[0]:.2f}, {weights[1]:.2f}, {weights[2]:.2f}\")\n","\n","        scheduler.step(valid_auc)\n","\n","        if valid_auc > best_valid_auc:\n","            best_valid_auc = valid_auc\n","            epochs_no_improve = 0\n","            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n","            print(f\"  -> New best model saved to {MODEL_SAVE_PATH} (AUC: {best_valid_auc:.4f})\")\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= EARLY_STOP_PATIENCE:\n","            print(f\"\\n--- Early stopping at epoch {epoch+1} ---\")\n","            break\n","\n","    print(\"\\n--- Test Run Training Complete ---\")\n","\n","    # ... (Final evaluation block is unchanged) ...\n","    print(f\"Loading best model from {MODEL_SAVE_PATH} for final test evaluation...\")\n","    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n","    test_loss, test_mean_auc = eval_model(model, test_loader, print_scores=True)\n","    print(f\"\\n======================================\")\n","    print(f\" Final Test Loss: {test_loss.item():.4f}\")\n","    print(f\" Final Test Mean ROC-AUC: {test_mean_auc:.4f}\")\n","    print(f\"======================================\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"mtYF88_V_p45"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Graph Transformer with MolCLR\n","Adapted from https://github.com/yuyangw/MolCLR"],"metadata":{"id":"rl6mw6kuOfz5"}},{"cell_type":"markdown","source":["Dataloading for Tox21"],"metadata":{"id":"BiAwzur9ISiK"}},{"cell_type":"code","source":["import os\n","import csv\n","import math\n","import time\n","import random\n","import numpy as np\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","from torch_scatter import scatter\n","from torch_geometric.data import Data, Dataset\n","from torch_geometric.loader import DataLoader\n","import rdkit\n","from rdkit import Chem\n","from rdkit.Chem.rdchem import HybridizationType\n","from rdkit.Chem.rdchem import BondType as BT\n","from rdkit.Chem import AllChem\n","from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmiles\n","from rdkit import RDLogger\n","from openbabel import pybel\n","RDLogger.DisableLog('rdApp.*')\n","\n","\n","ATOM_LIST = list(range(1,119))\n","CHIRALITY_LIST = [\n","    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n","    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n","    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n","    Chem.rdchem.ChiralType.CHI_OTHER\n","]\n","BOND_LIST = [BT.SINGLE, BT.DOUBLE, BT.TRIPLE, BT.AROMATIC]\n","BONDDIR_LIST = [\n","    Chem.rdchem.BondDir.NONE,\n","    Chem.rdchem.BondDir.ENDUPRIGHT,\n","    Chem.rdchem.BondDir.ENDDOWNRIGHT\n","]\n","\n","def canonical(s):\n","    m = Chem.MolFromSmiles(s)\n","    return Chem.MolToSmiles(m, canonical=True) if m else None\n","\n","def _generate_scaffold(smiles, include_chirality=False):\n","    mol = Chem.MolFromSmiles(smiles)\n","    scaffold = MurckoScaffoldSmiles(mol=mol, includeChirality=include_chirality)\n","    return scaffold\n","\n","\n","def generate_scaffolds(dataset, log_every_n=1000):\n","    scaffolds = {}\n","    data_len = len(dataset)\n","    print(data_len)\n","\n","    print(\"About to generate scaffolds\")\n","    for ind, smiles in enumerate(dataset.smiles_data):\n","        if ind % log_every_n == 0:\n","            print(\"Generating scaffold %d/%d\" % (ind, data_len))\n","        scaffold = _generate_scaffold(smiles)\n","        if scaffold not in scaffolds:\n","            scaffolds[scaffold] = [ind]\n","        else:\n","            scaffolds[scaffold].append(ind)\n","\n","    # Sort from largest to smallest scaffold sets\n","    scaffolds = {key: sorted(value) for key, value in scaffolds.items()}\n","    scaffold_sets = [\n","        scaffold_set for (scaffold, scaffold_set) in sorted(\n","            scaffolds.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True)\n","    ]\n","    return scaffold_sets\n","\n","\n","def scaffold_split(dataset, valid_size, test_size, seed=None, log_every_n=1000):\n","    train_size = 1.0 - valid_size - test_size\n","    scaffold_sets = generate_scaffolds(dataset)\n","\n","    train_cutoff = train_size * len(dataset)\n","    valid_cutoff = (train_size + valid_size) * len(dataset)\n","    train_inds: List[int] = []\n","    valid_inds: List[int] = []\n","    test_inds: List[int] = []\n","\n","    print(\"About to sort in scaffold sets\")\n","    for scaffold_set in scaffold_sets:\n","        if len(train_inds) + len(scaffold_set) > train_cutoff:\n","            if len(train_inds) + len(valid_inds) + len(scaffold_set) > valid_cutoff:\n","                test_inds += scaffold_set\n","            else:\n","                valid_inds += scaffold_set\n","        else:\n","            train_inds += scaffold_set\n","    return train_inds, valid_inds, test_inds\n","\n","\n","def read_smiles(data_path, target, task):\n","    smiles_data, labels = [], []\n","    with open(data_path) as csv_file:\n","        csv_reader = csv.DictReader(csv_file, delimiter=',')\n","        for i, row in enumerate(csv_reader):\n","            if i != 0:\n","                smiles = row['smiles']\n","                label = row[target]\n","                mol = Chem.MolFromSmiles(smiles)\n","                if mol != None and label != '':\n","                    smiles_data.append(smiles)\n","                    if task == 'classification':\n","                        labels.append(int(label))\n","                    elif task == 'regression':\n","                        labels.append(float(label))\n","                    else:\n","                        ValueError('task must be either regression or classification')\n","    print(len(smiles_data))\n","    return smiles_data, labels\n","def generate_3d(mol):\n","    try:\n","        mol3d = Chem.AddHs(mol)\n","        params = AllChem.ETKDGv3()\n","\n","        result = AllChem.EmbedMolecule(mol3d, params)\n","\n","        if result != 0:\n","            return None, False\n","\n","        #AllChem.UFFOptimizeMolecule(mol3d)\n","        pos = mol3d.GetConformer().GetPositions()\n","        pos = torch.tensor(pos, dtype=torch.float)\n","        pos = pos - pos.mean(dim=0, keepdim=True)\n","        return pos,  True\n","\n","    except Exception as e:\n","        raise e\n","        return None, False\n","\n","def generate_3d_openbabel(smiles):\n","    try:\n","        mol = pybel.readstring(\"smi\", smiles)\n","        mol.make3D()\n","        coords = torch.tensor([list(a.coords) for a in mol.atoms], dtype=torch.float)\n","        return coords, True\n","    except Exception:\n","\n","        return None, False\n","\n","class MolTestDataset(Dataset):\n","    def __init__(self, data_path, target, task, use3D=False):\n","        super(Dataset, self).__init__()\n","        #self.smiles_data, self.labels = read_smiles(data_path, target, task)\n","\n","\n","        self.use3D= use3D\n","        smiles_list, labels = read_smiles(data_path, target, task)\n","        filtered_smiles, filtered_labels = [], []\n","        invalid_count = 0\n","\n","        for s,l in zip(smiles_list,labels):\n","            try:\n","                mol = Chem.MolFromSmiles(s)\n","                mol = Chem.AddHs(mol)\n","            except:\n","                invalid_smiles += 1\n","                continue\n","\n","            if use3D:\n","                pos, flag = generate_3d(mol)\n","\n","                if not flag or pos is None:\n","                    invalid_count += 1\n","                    continue\n","\n","                filtered_smiles.append((s, pos))\n","                filtered_labels.append(l)\n","\n","            else:\n","                filtered_smiles.append(s)\n","                filtered_labels.append(l)\n","\n","        self.smiles_data = filtered_smiles\n","        self.labels = filtered_labels\n","        print(f\" Filtered out {invalid_count} invalid SMILES.\")\n","\n","\n","\n","        self.task = task\n","\n","        self.conversion = 1\n","\n","    def __getitem__(self, index):\n","        if self.use3D:\n","            s, pos =  self.smiles_data[index]\n","        else:\n","            s =  self.smiles_data[index]\n","\n","        mol = Chem.MolFromSmiles(s)\n","        mol = Chem.AddHs(mol)\n","\n","        N = mol.GetNumAtoms()\n","        M = mol.GetNumBonds()\n","\n","        type_idx = []\n","        chirality_idx = []\n","        atomic_number = []\n","        for atom in mol.GetAtoms():\n","            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n","            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n","            atomic_number.append(atom.GetAtomicNum())\n","\n","        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n","        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n","        x = torch.cat([x1, x2], dim=-1)\n","\n","        row, col, edge_feat = [], [], []\n","        for bond in mol.GetBonds():\n","            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n","            row += [start, end]\n","            col += [end, start]\n","            edge_feat.append([\n","                BOND_LIST.index(bond.GetBondType()),\n","                BONDDIR_LIST.index(bond.GetBondDir())\n","            ])\n","            edge_feat.append([\n","                BOND_LIST.index(bond.GetBondType()),\n","                BONDDIR_LIST.index(bond.GetBondDir())\n","            ])\n","\n","        edge_index = torch.tensor([row, col], dtype=torch.long)\n","        edge_attr = torch.tensor(np.array(edge_feat), dtype=torch.long)\n","        if self.task == 'classification':\n","            y = torch.tensor(self.labels[index], dtype=torch.long).view(1,-1)\n","        data = Data(x=x, y=y, edge_index=edge_index, edge_attr=edge_attr)\n","        if self.use3D:\n","            if pos is None:\n","                raise RuntimeError(f\"pos became None at index {index}, SMILES={s}\")\n","            data.pos = pos\n","\n","\n","        return data\n","\n","    def __len__(self):\n","        return len(self.smiles_data)\n","\n","\n","class MolTestDatasetWrapper(object):\n","\n","    def __init__(self,\n","        batch_size, num_workers, valid_size, test_size,\n","        data_path, target, task, splitting, use_3D= False\n","    ):\n","        super(object, self).__init__()\n","        self.data_path = data_path\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.valid_size = valid_size\n","        self.test_size = test_size\n","        self.target = target\n","        self.task = task\n","        self.splitting = splitting\n","        self.use_3D= use_3D\n","        assert splitting in ['random', 'scaffold', 'tox21']\n","\n","    def _load_tox21_split(self, dataset):\n","        \"\"\"\n","        Read train_smiles.txt, valid_smiles.txt, test_smiles.txt\n","        and map each SMILES to index in dataset.smiles_data\n","        \"\"\"\n","        def read_txt(path):\n","            with open(path) as f:\n","                return [l.strip() for l in f if l.strip()]\n","\n","        root = self.data_path\n","        directory = os.path.dirname(root)\n","\n","        train_smiles = read_txt(os.path.join(directory, \"train_smiles.txt\"))\n","        valid_smiles = read_txt(os.path.join(directory, \"valid_smiles.txt\"))\n","        test_smiles  = read_txt(os.path.join(directory, \"test_smiles.txt\"))\n","\n","        # Map SMILES ? list index in dataset\n","        if self.use_3D:\n","            smiles_to_index = {canonical(s): i for i, (s,_) in enumerate(dataset.smiles_data)}\n","        else:\n","            smiles_to_index = {canonical(s): i for i, s in enumerate(dataset.smiles_data)}\n","\n","        train_idx = [smiles_to_index[s] for s in train_smiles if s in smiles_to_index]\n","        valid_idx = [smiles_to_index[s] for s in valid_smiles if s in smiles_to_index]\n","        test_idx  = [smiles_to_index[s] for s in test_smiles  if s in smiles_to_index]\n","\n","        print(f\"train={len(train_idx)}, valid={len(valid_idx)}, test={len(test_idx)}\")\n","\n","        return train_idx, valid_idx, test_idx\n","\n","    def get_data_loaders(self):\n","        train_dataset = MolTestDataset(data_path=self.data_path, target=self.target, task=self.task, use3D=self.use_3D)\n","        train_loader, valid_loader, test_loader = self.get_train_validation_data_loaders(train_dataset)\n","        return train_loader, valid_loader, test_loader\n","\n","    def get_train_validation_data_loaders(self, train_dataset):\n","        if self.splitting == 'random':\n","            # obtain training indices that will be used for validation\n","            num_train = len(train_dataset)\n","            indices = list(range(num_train))\n","            np.random.shuffle(indices)\n","\n","            split = int(np.floor(self.valid_size * num_train))\n","            split2 = int(np.floor(self.test_size * num_train))\n","            valid_idx, test_idx, train_idx = indices[:split], indices[split:split+split2], indices[split+split2:]\n","\n","        elif self.splitting == 'scaffold':\n","            train_idx, valid_idx, test_idx = scaffold_split(train_dataset, self.valid_size, self.test_size)\n","\n","        elif self.splitting == 'tox21':\n","            train_idx, valid_idx, test_idx = self._load_tox21_split(train_dataset)\n","\n","\n","        # define samplers for obtaining training and validation batches\n","        train_sampler = SubsetRandomSampler(train_idx)\n","        valid_sampler = SubsetRandomSampler(valid_idx)\n","        test_sampler = SubsetRandomSampler(test_idx)\n","\n","        train_loader = DataLoader(\n","            train_dataset, batch_size=self.batch_size, sampler=train_sampler,\n","            num_workers=self.num_workers, drop_last=False\n","        )\n","        valid_loader = DataLoader(\n","            train_dataset, batch_size=self.batch_size, sampler=valid_sampler,\n","            num_workers=self.num_workers, drop_last=False\n","        )\n","        test_loader = DataLoader(\n","            train_dataset, batch_size=self.batch_size, sampler=test_sampler,\n","            num_workers=self.num_workers, drop_last=False\n","        )\n","\n","        return train_loader, valid_loader, test_loader\n"],"metadata":{"id":"ahE2RfCfI77Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Graph Transformer Model"],"metadata":{"id":"ebuzTQw5ISp2"}},{"cell_type":"code","source":["import math\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import TransformerConv, global_add_pool, global_mean_pool, global_max_pool\n","\n","num_atom_type = 119\n","num_chirality_tag = 3\n","num_bond_type = 5\n","num_bond_direction = 3\n","\n","\n","class GraphTransformer(nn.Module):\n","    def __init__(\n","        self,\n","        task='classification',\n","        num_layer=5,\n","        emb_dim=300,\n","        feat_dim=256,\n","        heads=4,\n","        drop_ratio=0.1,\n","        pool='mean',\n","        edge_emb_dim=32,\n","        use_3D = False\n","    ):\n","        super(GraphTransformer, self).__init__()\n","        self.num_layer = num_layer\n","        self.emb_dim = emb_dim\n","        self.feat_dim = feat_dim\n","        self.drop_ratio = drop_ratio\n","        self.heads = heads\n","        self.task = task\n","        self.edge_emb_dim = edge_emb_dim\n","        self.use_3D = use_3D\n","\n","        if self.num_layer < 2:\n","            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n","\n","\n","        self.x_embedding1 = nn.Embedding(num_atom_type, emb_dim)\n","        self.x_embedding2 = nn.Embedding(num_chirality_tag, emb_dim)\n","        nn.init.xavier_uniform_(self.x_embedding1.weight)\n","        nn.init.xavier_uniform_(self.x_embedding2.weight)\n","\n","\n","        self.edge_embedding1 = nn.Embedding(num_bond_type, edge_emb_dim)\n","        self.edge_embedding2 = nn.Embedding(num_bond_direction, edge_emb_dim)\n","        nn.init.xavier_uniform_(self.edge_embedding1.weight)\n","        nn.init.xavier_uniform_(self.edge_embedding2.weight)\n","        self.edge_proj = nn.Linear(edge_emb_dim, edge_emb_dim)\n","\n","        if use_3D:\n","            self.pos_emb = nn.Linear(3, emb_dim)\n","\n","        self.layers = nn.ModuleList()\n","        self.norms1 = nn.ModuleList()\n","        self.norms2 = nn.ModuleList()\n","        self.ffns = nn.ModuleList()\n","\n","        for _ in range(num_layer):\n","            self.layers.append(\n","                TransformerConv(\n","                    in_channels=emb_dim,\n","                    out_channels=emb_dim // heads,\n","                    heads=heads,\n","                    dropout=drop_ratio,\n","                    edge_dim=edge_emb_dim,\n","                )\n","            )\n","            self.norms1.append(nn.LayerNorm(emb_dim))\n","            self.norms2.append(nn.LayerNorm(emb_dim))\n","            self.ffns.append(\n","                nn.Sequential(\n","                    nn.Linear(emb_dim, 4 * emb_dim),\n","                    nn.ReLU(inplace=True),\n","                    nn.Dropout(drop_ratio),\n","                    nn.Linear(4 * emb_dim, emb_dim),\n","                    nn.Dropout(drop_ratio),\n","                )\n","            )\n","\n","\n","        if pool == 'mean':\n","            self.pool = global_mean_pool\n","        elif pool == 'max':\n","            self.pool = global_max_pool\n","        else:\n","            raise ValueError(\"Not defined pooling!\")\n","\n","\n","        self.feat_lin = nn.Linear(self.emb_dim, self.feat_dim)\n","\n","        if self.task == 'classification':\n","            self.pred_head = nn.Sequential(\n","                nn.Linear(self.feat_dim, self.feat_dim // 2),\n","                nn.Softplus(),\n","                nn.Linear(self.feat_dim // 2, 2),\n","            )\n","        else:\n","            raise ValueError(\"task must be 'classification'\")\n","\n","    def _edge_encode(self, edge_attr):\n","        e = self.edge_embedding1(edge_attr[:, 0].long()) + \\\n","            self.edge_embedding2(edge_attr[:, 1].long())\n","        return self.edge_proj(e)\n","\n","    def forward(self, data):\n","        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n","\n","\n","        if self.use_3D:\n","            pos= data.pos\n","            pe = self.pos_emb(pos)\n","        else:\n","            pe = 0\n","\n","\n","        h = self.x_embedding1(x[:, 0].long()) + self.x_embedding2(x[:, 1].long())\n","        h = h + pe\n","        edge_feat = self._edge_encode(edge_attr)\n","\n","\n","        for conv, norm1, norm2, ffn in zip(self.layers, self.norms1, self.norms2, self.ffns):\n","            h_res = h\n","            h = conv(h, edge_index, edge_attr=edge_feat)\n","            h = F.dropout(h, p=self.drop_ratio, training=self.training)\n","            h = norm1(h_res + h)\n","\n","            h_res2 = h\n","            h = ffn(h)\n","            h = norm2(h_res2 + h)\n","\n","\n","        h = self.pool(h, batch)\n","        h = self.feat_lin(h)\n","\n","        return h, self.pred_head(h)\n","\n","    def load_my_state_dict(self, state_dict):\n","        own_state = self.state_dict()\n","        for name, param in state_dict.items():\n","            if name not in own_state:\n","                continue\n","            if isinstance(param, nn.parameter.Parameter):\n","                param = param.data\n","            own_state[name].copy_(param)\n"],"metadata":{"id":"torczo3yITNv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finetuning Loop for Pretrained Graph Transformer"],"metadata":{"id":"SsWlPl30MVzI"}},{"cell_type":"code","source":["import os\n","import shutil\n","import sys\n","import yaml\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error\n","\n","def _save_config_file(model_checkpoints_folder):\n","    if not os.path.exists(model_checkpoints_folder):\n","        os.makedirs(model_checkpoints_folder)\n","        shutil.copy('./config_finetune.yaml', os.path.join(model_checkpoints_folder, 'config_finetune.yaml'))\n","\n","class Normalizer(object):\n","\n","    def __init__(self, tensor):\n","        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n","        self.mean = torch.mean(tensor)\n","        self.std = torch.std(tensor)\n","\n","    def norm(self, tensor):\n","        return (tensor - self.mean) / self.std\n","\n","    def denorm(self, normed_tensor):\n","        return normed_tensor * self.std + self.mean\n","\n","    def state_dict(self):\n","        return {'mean': self.mean,\n","                'std': self.std}\n","\n","    def load_state_dict(self, state_dict):\n","        self.mean = state_dict['mean']\n","        self.std = state_dict['std']\n","\n","\n","class FineTune(object):\n","    def __init__(self, dataset, config):\n","        self.config = config\n","        self.device = self._get_device()\n","\n","        current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n","        dir_name = current_time + '_' + config['task_name'] + '_' + config['dataset']['target']\n","        log_dir = os.path.join('finetune', dir_name)\n","        self.writer = SummaryWriter(log_dir=log_dir)\n","        self.dataset = dataset\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def _get_device(self):\n","        if torch.cuda.is_available() and self.config['gpu'] != 'cpu':\n","            device = self.config['gpu']\n","            torch.cuda.set_device(device)\n","        else:\n","            device = 'cpu'\n","        print(\"Running on:\", device)\n","\n","        return device\n","\n","    def _step(self, model, data, n_iter):\n","        # get the prediction\n","        __, pred = model(data)  # [N,C]\n","        loss = self.criterion(pred, data.y.flatten())\n","\n","        return loss\n","\n","    def train(self):\n","        train_loader, valid_loader, test_loader = self.dataset.get_data_loaders()\n","\n","        self.normalizer = None\n","\n","        model = GraphTransformer(**self.config[\"model\"]).to(self.device)\n","        model = self._load_pre_trained_weights(model)\n","        layer_list = []\n","        for name, param in model.named_parameters():\n","            if 'pred_head' in name:\n","                print(name, param.requires_grad)\n","                layer_list.append(name)\n","\n","        params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in layer_list, model.named_parameters()))))\n","        base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in layer_list, model.named_parameters()))))\n","\n","        optimizer = torch.optim.Adam(\n","            [{'params': base_params, 'lr': self.config['init_base_lr']}, {'params': params}],\n","            self.config['init_lr'], weight_decay=eval(self.config['weight_decay'])\n","        )\n","\n","        model_checkpoints_folder = os.path.join(self.writer.log_dir, 'checkpoints')\n","\n","        # save config file\n","        _save_config_file(model_checkpoints_folder)\n","\n","        n_iter = 0\n","        valid_n_iter = 0\n","        best_valid_loss = np.inf\n","        best_valid_rgr = np.inf\n","        best_valid_cls = 0\n","\n","        for epoch_counter in range(self.config['epochs']):\n","            for bn, data in enumerate(train_loader):\n","                optimizer.zero_grad()\n","\n","                data = data.to(self.device)\n","                loss = self._step(model, data, n_iter)\n","\n","                if n_iter % self.config['log_every_n_steps'] == 0:\n","                    self.writer.add_scalar('train_loss', loss, global_step=n_iter)\n","                    print(epoch_counter, bn, loss.item())\n","                loss.backward()\n","\n","                optimizer.step()\n","                n_iter += 1\n","\n","            # validate the model if requested\n","            if epoch_counter % self.config['eval_every_n_epochs'] == 0:\n","                if self.config['dataset']['task'] == 'classification':\n","                    valid_loss, valid_cls = self._validate(model, valid_loader)\n","                    if valid_cls > best_valid_cls:\n","                        # save the model weights\n","                        best_valid_cls = valid_cls\n","                        torch.save(model.state_dict(), os.path.join(model_checkpoints_folder, 'model.pth'))\n","\n","                self.writer.add_scalar('validation_loss', valid_loss, global_step=valid_n_iter)\n","                valid_n_iter += 1\n","\n","        self._test(model, test_loader)\n","\n","    def _load_pre_trained_weights(self, model):\n","        try:\n","            checkpoints_folder = os.path.join('./ckpt', self.config['fine_tune_from'], 'checkpoints')\n","            state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location=self.device)\n","            # model.load_state_dict(state_dict)\n","            model.load_my_state_dict(state_dict)\n","            print(\"Loaded pre-trained model with success.\")\n","        except FileNotFoundError:\n","            print(\"Pre-trained weights not found. Training from scratch.\")\n","\n","        return model\n","\n","    def _validate(self, model, valid_loader):\n","        predictions = []\n","        labels = []\n","        with torch.no_grad():\n","            model.eval()\n","\n","            valid_loss = 0.0\n","            num_data = 0\n","            for bn, data in enumerate(valid_loader):\n","                data = data.to(self.device)\n","\n","                __, pred = model(data)\n","                loss = self._step(model, data, bn)\n","\n","                valid_loss += loss.item() * data.y.size(0)\n","                num_data += data.y.size(0)\n","\n","                if self.normalizer:\n","                    pred = self.normalizer.denorm(pred)\n","\n","                if self.config['dataset']['task'] == 'classification':\n","                    pred = F.softmax(pred, dim=-1)\n","\n","                if self.device == 'cpu':\n","                    predictions.extend(pred.detach().numpy())\n","                    labels.extend(data.y.flatten().numpy())\n","                else:\n","                    predictions.extend(pred.cpu().detach().numpy())\n","                    labels.extend(data.y.cpu().flatten().numpy())\n","\n","            valid_loss /= num_data\n","\n","        model.train()\n","        predictions = np.array(predictions)\n","        labels = np.array(labels)\n","        roc_auc = roc_auc_score(labels, predictions[:,1])\n","        print('Validation loss:', valid_loss, 'ROC AUC:', roc_auc)\n","        return valid_loss, roc_auc\n","\n","    def _test(self, model, test_loader):\n","        model_path = os.path.join(self.writer.log_dir, 'checkpoints', 'model.pth')\n","        state_dict = torch.load(model_path, map_location=self.device)\n","        model.load_state_dict(state_dict)\n","        print(\"Loaded trained model with success.\")\n","\n","        # test steps\n","        predictions = []\n","        labels = []\n","        with torch.no_grad():\n","            model.eval()\n","\n","            test_loss = 0.0\n","            num_data = 0\n","            for bn, data in enumerate(test_loader):\n","                data = data.to(self.device)\n","\n","                __, pred = model(data)\n","                loss = self._step(model, data, bn)\n","\n","                test_loss += loss.item() * data.y.size(0)\n","                num_data += data.y.size(0)\n","\n","                if self.normalizer:\n","                    pred = self.normalizer.denorm(pred)\n","\n","                if self.config['dataset']['task'] == 'classification':\n","                    pred = F.softmax(pred, dim=-1)\n","\n","                if self.device == 'cpu':\n","                    predictions.extend(pred.detach().numpy())\n","                    labels.extend(data.y.flatten().numpy())\n","                else:\n","                    predictions.extend(pred.cpu().detach().numpy())\n","                    labels.extend(data.y.cpu().flatten().numpy())\n","\n","            test_loss /= num_data\n","\n","        model.train()\n","\n","        predictions = np.array(predictions)\n","        labels = np.array(labels)\n","        self.roc_auc = roc_auc_score(labels, predictions[:,1])\n","        print('Test loss:', test_loss, 'Test ROC AUC:', self.roc_auc)\n","\n","\n","def main(config):\n","    dataset = MolTestDatasetWrapper(config['batch_size'], **config['dataset'])\n","\n","    fine_tune = FineTune(dataset, config)\n","    fine_tune.train()\n","\n","    if config['dataset']['task'] == 'classification':\n","        return fine_tune.roc_auc\n","\n","\n","if __name__ == \"__main__\":\n","    config = yaml.load(open(\"config_finetune.yaml\", \"r\"), Loader=yaml.FullLoader)\n","\n","\n","    if config['task_name'] == 'Tox21':\n","        config['dataset']['task'] = 'classification'\n","        config['dataset']['data_path'] = 'data/tox21/tox21.csv'\n","        target_list = [\n","            \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\", \"NR-ER\", \"NR-ER-LBD\",\n","            \"NR-PPAR-gamma\", \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n","        ]\n","\n","    else:\n","        raise ValueError('Undefined downstream task!')\n","\n","    results_list = []\n","    for target in target_list:\n","        config['dataset']['target'] = target\n","        result = main(config)\n","        results_list.append([target, result])\n","\n","    os.makedirs('experiments', exist_ok=True)\n","    df = pd.DataFrame(results_list)\n","    df.to_csv(\n","        'experiments/{}_{}_finetune.csv'.format(config['fine_tune_from'], config['task_name']),\n","        mode='a', index=False, header=False\n","    )"],"metadata":{"id":"aG42uI9TMV7z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GINE Assay multi head cross attention"],"metadata":{"id":"6SK7qfR0_K7Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NADH9ALWqbbo"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","from rdkit import Chem\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.nn import GINEConv, global_mean_pool, global_max_pool\n","\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","\n","from sentence_transformers import SentenceTransformer\n","\n","\n","# ============================================================\n","# 0. CONFIG\n","# ============================================================\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","DATA_DIR = r\"E:\\graphml project\\novel\\processed\"\n","\n","TRAIN_CSV  = os.path.join(DATA_DIR, \"train_clean.csv\")\n","VAL_CSV    = os.path.join(DATA_DIR, \"val_clean.csv\")\n","TEST_CSV   = os.path.join(DATA_DIR, \"test_clean.csv\")\n","\n","TRAIN_DESC = os.path.join(DATA_DIR, \"train_rdkit_desc.npz\")\n","VAL_DESC   = os.path.join(DATA_DIR, \"val_rdkit_desc.npz\")\n","TEST_DESC  = os.path.join(DATA_DIR, \"test_rdkit_desc.npz\")\n","\n","ASSAYS  = [\n","    \"NR-AR\",\"NR-AR-LBD\",\"NR-AhR\",\"NR-Aromatase\",\n","    \"NR-ER\",\"NR-ER-LBD\",\"NR-PPAR-gamma\",\n","    \"SR-ARE\",\"SR-ATAD5\",\"SR-HSE\",\"SR-MMP\",\"SR-p53\"\n","]\n","WEIGHTS = [f\"w{i}\" for i in range(1, 13)]\n","N_TASKS = len(ASSAYS)\n","\n","# Natural language descriptions for each assay (for text prompts)\n","ASSAY_DESCRIPTIONS = {\n","    \"NR-AR\":          \"androgen receptor binding and endocrine disruption potential\",\n","    \"NR-AR-LBD\":      \"androgen receptor ligand binding domain interaction\",\n","    \"NR-AhR\":         \"aryl hydrocarbon receptor activation and xenobiotic metabolism\",\n","    \"NR-Aromatase\":   \"aromatase enzyme inhibition and steroid metabolism disruption\",\n","    \"NR-ER\":          \"estrogen receptor binding and hormonal activity modulation\",\n","    \"NR-ER-LBD\":      \"estrogen receptor ligand binding domain interaction\",\n","    \"NR-PPAR-gamma\":  \"peroxisome proliferator activated receptor gamma activation\",\n","    \"SR-ARE\":         \"antioxidant response element activation and oxidative stress response\",\n","    \"SR-ATAD5\":       \"ATAD5 biomarker response indicating genotoxicity\",\n","    \"SR-HSE\":         \"heat shock response element activation and protein stress\",\n","    \"SR-MMP\":         \"mitochondrial membrane potential disruption and cytotoxicity\",\n","    \"SR-p53\":         \"p53 tumor suppressor pathway activation and DNA damage response\"\n","}\n","\n","\n","# ============================================================\n","# 1. CHEM â†’ GRAPH HELPERS\n","# ============================================================\n","\n","def atom_features(atom):\n","    \"\"\"Simple numeric atom features.\"\"\"\n","    return np.array([\n","        atom.GetAtomicNum(),       # Z\n","        atom.GetTotalDegree(),     # degree\n","        atom.GetFormalCharge(),    # charge\n","        atom.GetTotalNumHs(),      # attached Hs\n","        int(atom.GetIsAromatic())  # aromatic flag\n","    ], dtype=np.float32)\n","\n","\n","def bond_features(bond):\n","    \"\"\"Edge features for GINE (numeric).\"\"\"\n","    bt = bond.GetBondType()\n","    bond_type = {\n","        Chem.BondType.SINGLE: 1.0,\n","        Chem.BondType.DOUBLE: 2.0,\n","        Chem.BondType.TRIPLE: 3.0,\n","        Chem.BondType.AROMATIC: 1.5,\n","    }.get(bt, 0.0)\n","    return np.array([\n","        bond_type,\n","        float(bond.GetIsConjugated()),\n","        float(bond.IsInRing())\n","    ], dtype=np.float32)\n","\n","\n","def smiles_to_graph_with_pe(smiles, y_vec, w_vec):\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None: return None\n","\n","\n","    atom_feats = []\n","    for atom in mol.GetAtoms():\n","        atom_feats.append(atom_features(atom))\n","    x = torch.tensor(np.stack(atom_feats, axis=0), dtype=torch.float)\n","\n","    rows, cols, eattr = [], [], []\n","    for b in mol.GetBonds():\n","        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n","        bf = bond_features(b)\n","        rows += [i, j]; cols += [j, i]\n","        eattr += [bf, bf]\n","\n","    edge_index = torch.tensor([rows, cols], dtype=torch.long)\n","    edge_attr = torch.tensor(np.stack(eattr, axis=0), dtype=torch.float) if len(eattr) > 0 else torch.zeros((0, 6))\n","\n","    data = Data(\n","        x=x,\n","        edge_index=edge_index,\n","        edge_attr=edge_attr,\n","        y=torch.tensor(y_vec, dtype=torch.float).view(1, -1),\n","        weight=torch.tensor(w_vec, dtype=torch.float).view(1, -1)\n","    )\n","\n","    # 2. APPLY POSITIONAL ENCODING (The New Part)\n","    # This adds a 'data.pe' attribute of shape [num_atoms, 20]\n","    data = pe_transform(data)\n","\n","    return data\n","\n","\n","# ============================================================\n","# 2. LOAD CSV + DESCRIPTORS â†’ GRAPH DATASETS\n","# ============================================================\n","\n","def load_split(csv_path, desc_path):\n","    df = pd.read_csv(csv_path)\n","    desc_npz = np.load(desc_path)[\"X\"]  # [N, D]\n","\n","    # Which SMILES column?\n","    if \"smiles_canonical\" in df.columns:\n","        smi_col = \"smiles_canonical\"\n","    elif \"smiles\" in df.columns:\n","        smi_col = \"smiles\"\n","    else:\n","        raise ValueError(\"No 'smiles' or 'smiles_canonical' column found.\")\n","\n","    assert len(df) == desc_npz.shape[0], \"CSV and desc shapes mismatch.\"\n","\n","    graphs = []\n","    skipped = 0\n","    for i, row in df.iterrows():\n","        smiles = str(row[smi_col])\n","        y_vec  = row[ASSAYS].astype(float).values\n","        w_vec  = row[WEIGHTS].astype(float).values\n","        d_vec  = desc_npz[i]\n","\n","        g = smiles_to_data(smiles, y_vec, w_vec, d_vec)\n","        if g is None:\n","            skipped += 1\n","            continue\n","        graphs.append(g)\n","\n","    print(f\"[{os.path.basename(csv_path)}] Loaded {len(graphs)} graphs, skipped {skipped} invalid.\")\n","    return graphs, desc_npz\n","\n","\n","print(\"Loading datasets...\")\n","train_graphs, train_desc = load_split(TRAIN_CSV, TRAIN_DESC)\n","val_graphs,   val_desc   = load_split(VAL_CSV,   VAL_DESC)\n","test_graphs,  test_desc  = load_split(TEST_CSV,  TEST_DESC)\n","\n","# ---- Standardize descriptor features (important!) ----\n","desc_mean = train_desc.mean(axis=0, keepdims=True)\n","desc_std  = train_desc.std(axis=0, keepdims=True) + 1e-8\n","\n","train_desc_norm = (train_desc - desc_mean) / desc_std\n","val_desc_norm   = (val_desc   - desc_mean) / desc_std\n","test_desc_norm  = (test_desc  - desc_mean) / desc_std\n","\n","# Attach normalized desc back to each Data object\n","for i, g in enumerate(train_graphs):\n","    g.desc_features = torch.tensor(train_desc_norm[i], dtype=torch.float)\n","for i, g in enumerate(val_graphs):\n","    g.desc_features = torch.tensor(val_desc_norm[i], dtype=torch.float)\n","for i, g in enumerate(test_graphs):\n","    g.desc_features = torch.tensor(test_desc_norm[i], dtype=torch.float)\n","\n","# Descriptor dimension\n","desc_dim = train_graphs[0].desc_features.size(0)\n","print(\"Descriptor dim:\", desc_dim)\n","\n","\n","# ============================================================\n","# 3. DATALOADERS\n","# ============================================================\n","\n","BATCH_SIZE = 64\n","\n","train_loader = DataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader   = DataLoader(val_graphs,   batch_size=BATCH_SIZE, shuffle=False)\n","test_loader  = DataLoader(test_graphs,  batch_size=BATCH_SIZE, shuffle=False)\n","\n","\n","# ============================================================\n","# 4. BUILD TEXT PROMPT EMBEDDINGS (SentenceTransformer)\n","# ============================================================\n","\n","print(\"Encoding assay descriptions with SentenceTransformer...\")\n","text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 384-dim\n","\n","assay_names = ASSAYS  # same order as labels\n","descriptions = [ASSAY_DESCRIPTIONS[a] for a in assay_names]\n","text_embs = text_model.encode(descriptions, convert_to_numpy=True)  # [12, 384]\n","text_embs = torch.tensor(text_embs, dtype=torch.float)\n","text_dim = text_embs.shape[1]\n","print(f\"Text embedding dim: {text_dim}\")\n","\n","\n","# ============================================================\n","# 5. GINE + ASSAY-PROMPT CROSS-ATTENTION MODEL\n","# ============================================================\n","\n","class GINECrossAttention(nn.Module):\n","    def __init__(self, node_feature_dim, edge_feature_dim,\n","                 desc_feature_dim, n_tasks,\n","                 text_dim=384, gnn_hidden=128,\n","                 initial_prompts=None):\n","        super().__init__()\n","\n","        self.n_tasks = n_tasks\n","        self.desc_feature_dim = desc_feature_dim\n","\n","        # ----- 1. GINE backbone -----\n","        nn1 = nn.Sequential(\n","            nn.Linear(node_feature_dim, gnn_hidden),\n","            nn.ReLU(),\n","            nn.Linear(gnn_hidden, gnn_hidden),\n","            nn.ReLU(),\n","        )\n","        self.conv1 = GINEConv(nn1, edge_dim=edge_feature_dim)\n","\n","        nn2 = nn.Sequential(\n","            nn.Linear(gnn_hidden, gnn_hidden),\n","            nn.ReLU(),\n","            nn.Linear(gnn_hidden, gnn_hidden),\n","            nn.ReLU(),\n","        )\n","        self.conv2 = GINEConv(nn2, edge_dim=edge_feature_dim)\n","\n","        self.bn1 = nn.BatchNorm1d(gnn_hidden)\n","        self.bn2 = nn.BatchNorm1d(gnn_hidden)\n","        self.gnn_dropout = 0.2\n","\n","        self.gnn_proj = nn.Sequential(\n","            nn.Linear(gnn_hidden * 2, gnn_hidden),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","        )\n","\n","        # ----- 2. Assay prompt embeddings (initialized from text) -----\n","        if initial_prompts is None:\n","            self.assay_prompts = nn.Parameter(torch.randn(n_tasks, text_dim))\n","        else:\n","            assert initial_prompts.shape == (n_tasks, text_dim)\n","            self.assay_prompts = nn.Parameter(initial_prompts.clone())\n","\n","        self.text_proj = nn.Sequential(\n","            nn.Linear(text_dim, gnn_hidden),\n","            nn.LayerNorm(gnn_hidden),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","        )\n","\n","        # ----- 3. Cross-attention -----\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=gnn_hidden,\n","            num_heads=8,\n","            dropout=0.1,\n","            batch_first=True,\n","        )\n","        self.layer_norm1 = nn.LayerNorm(gnn_hidden)\n","        self.layer_norm2 = nn.LayerNorm(gnn_hidden)\n","        self.cross_ffn = nn.Sequential(\n","            nn.Linear(gnn_hidden, gnn_hidden * 2),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(gnn_hidden * 2, gnn_hidden),\n","        )\n","\n","        # ----- 4. Classifier (cross + descriptors) -----\n","        classifier_dim = gnn_hidden + desc_feature_dim\n","        self.classifier = nn.Sequential(\n","            nn.Linear(classifier_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 256),\n","            nn.LayerNorm(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(256, n_tasks),\n","        )\n","\n","    def encode_graph(self, data):\n","        x, edge_index, edge_attr, batch = (\n","            data.x, data.edge_index, data.edge_attr, data.batch\n","        )\n","\n","        h = self.conv1(x, edge_index, edge_attr)\n","        h = self.bn1(h)\n","        h = F.elu(h)\n","        h = F.dropout(h, p=self.gnn_dropout, training=self.training)\n","\n","        h = self.conv2(h, edge_index, edge_attr)\n","        h = self.bn2(h)\n","        h = F.elu(h)\n","\n","        h_mean = global_mean_pool(h, batch)\n","        h_max  = global_max_pool(h, batch)\n","        h_cat  = torch.cat([h_mean, h_max], dim=1)\n","\n","        gnn_feat = self.gnn_proj(h_cat)   # [B, g_hidden]\n","        return gnn_feat\n","\n","    def forward(self, data, assay_attention=None):\n","        B = data.num_graphs\n","\n","        # 1. Graph encoding\n","        gnn_feat = self.encode_graph(data)   # [B, g_hidden]\n","\n","        # 2. Descriptor features\n","        desc_feat = data.desc_features.view(B, -1)   # [B, desc_dim]\n","\n","        # 3. Assay attention: shape [B, n_tasks]\n","        if assay_attention is None:\n","            assay_attention = torch.ones(B, self.n_tasks, device=gnn_feat.device) / self.n_tasks\n","\n","        # 4. Prompt mixing: weighted sum of assay prompt embeddings\n","        # assay_prompts: [n_tasks, text_dim], assay_attention: [B, n_tasks]\n","        text_prompts = torch.einsum('bi,ij->bj', assay_attention, self.assay_prompts)  # [B, text_dim]\n","        text_feat = self.text_proj(text_prompts)  # [B, g_hidden]\n","\n","        # 5. Cross-attention: text queries attend to graph representation\n","        text_q   = text_feat.unsqueeze(1)   # [B, 1, g_hidden]\n","        graph_kv = gnn_feat.unsqueeze(1)    # [B, 1, g_hidden]\n","\n","        attended, _ = self.cross_attention(\n","            query=text_q,\n","            key=graph_kv,\n","            value=graph_kv,\n","        )\n","        attended = attended.squeeze(1)   # [B, g_hidden]\n","\n","        # 6. Residual + FFN\n","        cross_feat = self.layer_norm1(text_feat + attended)\n","        cross_feat = self.layer_norm2(cross_feat + self.cross_ffn(cross_feat))\n","\n","        # 7. Concatenate with descriptors and classify\n","        combined = torch.cat([cross_feat, desc_feat], dim=1)  # [B, g_hidden + desc_dim]\n","        logits = self.classifier(combined)                   # [B, n_tasks]\n","        return logits\n","\n","\n","# ============================================================\n","# 6. ASYMMETRIC LOSS FOR IMBALANCE\n","# ============================================================\n","\n","class AsymmetricLoss(nn.Module):\n","    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8):\n","        super().__init__()\n","        self.gamma_neg = gamma_neg\n","        self.gamma_pos = gamma_pos\n","        self.clip = clip\n","        self.eps = eps\n","\n","    def forward(self, logits, targets, weights):\n","        \"\"\"\n","        logits, targets, weights: [B, T]\n","        weights is used as mask (1 where label present, 0 where missing)\n","        \"\"\"\n","        x_sigmoid = torch.sigmoid(logits)\n","        xs_pos = x_sigmoid\n","        xs_neg = 1 - x_sigmoid\n","\n","        if self.clip is not None and self.clip > 0:\n","            xs_neg = (xs_neg + self.clip).clamp(max=1.0)\n","\n","        los_pos = targets * torch.log(xs_pos.clamp(min=self.eps))\n","        los_neg = (1.0 - targets) * torch.log(xs_neg.clamp(min=self.eps))\n","        loss = los_pos + los_neg\n","\n","        pt0 = xs_neg ** self.gamma_neg\n","        pt1 = xs_pos ** self.gamma_pos\n","        loss = loss * (pt0 + pt1)\n","\n","        weighted_loss = -loss * weights\n","        return weighted_loss.sum() / (weights.sum() + 1e-8)\n","\n","\n","# ============================================================\n","# 7. TRAIN & EVAL FUNCTIONS\n","# ============================================================\n","\n","def train_epoch(loader, model, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    total_graphs = 0\n","\n","    for batch in loader:\n","        batch = batch.to(device)\n","        B = batch.num_graphs\n","\n","        y = batch.y.float().view(B, N_TASKS)\n","        w = batch.weight.float().view(B, N_TASKS)\n","        mask = (w > 0).float()\n","\n","        # Assay attention: emphasize labeled assays\n","        if mask.sum() > 0:\n","            assay_attention = mask / mask.sum(dim=1, keepdim=True).clamp(min=1e-8)\n","        else:\n","            assay_attention = torch.ones(B, N_TASKS, device=device) / N_TASKS\n","\n","        logits = model(batch, assay_attention)   # [B, T]\n","        loss = criterion(logits, y, mask)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        total_loss += loss.item() * B\n","        total_graphs += B\n","\n","    return total_loss / total_graphs\n","\n","\n","def evaluate(loader, model):\n","    model.eval()\n","    all_probs = []\n","    all_labels = []\n","    all_weights = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch = batch.to(device)\n","            B = batch.num_graphs\n","\n","            y = batch.y.float().view(B, N_TASKS)\n","            w = batch.weight.float().view(B, N_TASKS)\n","\n","            # Uniform attention during eval\n","            assay_attention = torch.ones(B, N_TASKS, device=device) / N_TASKS\n","            logits = model(batch, assay_attention)\n","            probs  = torch.sigmoid(logits)\n","\n","            all_probs.append(probs.cpu())\n","            all_labels.append(y.cpu())\n","            all_weights.append(w.cpu())\n","\n","    probs  = torch.cat(all_probs, dim=0).numpy()\n","    labels = torch.cat(all_labels, dim=0).numpy()\n","    weights= torch.cat(all_weights, dim=0).numpy()\n","\n","    roc_scores = {}\n","    pr_scores  = {}\n","\n","    for j, assay in enumerate(ASSAYS):\n","        mask = weights[:, j] > 0\n","        if mask.sum() < 5:\n","            roc_scores[assay] = np.nan\n","            pr_scores[assay]  = np.nan\n","            continue\n","\n","        y_true = labels[mask, j]\n","        y_pred = probs[mask, j]\n","\n","        try:\n","            roc_scores[assay] = roc_auc_score(y_true, y_pred)\n","            pr_scores[assay]  = average_precision_score(y_true, y_pred)\n","        except ValueError:\n","            roc_scores[assay] = np.nan\n","            pr_scores[assay]  = np.nan\n","\n","    mean_roc = np.nanmean(list(roc_scores.values()))\n","    mean_pr  = np.nanmean(list(pr_scores.values()))\n","    return roc_scores, pr_scores, mean_roc, mean_pr\n","\n","\n","# ============================================================\n","# 8. MAIN TRAIN LOOP\n","# ============================================================\n","\n","def main():\n","    EPOCHS = 60\n","    LR = 2e-4\n","    WEIGHT_DECAY = 1e-4\n","\n","    sample = train_graphs[0]\n","    node_dim = sample.x.size(1)\n","    edge_dim = sample.edge_attr.size(1)\n","    print(\"Node dim:\", node_dim, \"| Edge dim:\", edge_dim)\n","\n","    model = GINECrossAttention(\n","        node_feature_dim=node_dim,\n","        edge_feature_dim=edge_dim,\n","        desc_feature_dim=desc_dim,\n","        n_tasks=N_TASKS,\n","        text_dim=text_dim,\n","        gnn_hidden=128,\n","        initial_prompts=text_embs  # <-- text-initialized prompts\n","    ).to(device)\n","\n","    print(\"Model params:\", sum(p.numel() for p in model.parameters()))\n","\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=LR,\n","        weight_decay=WEIGHT_DECAY,\n","        betas=(0.9, 0.95),\n","    )\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=EPOCHS, eta_min=1e-6\n","    )\n","    criterion = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05)\n","\n","    best_val_roc = -1.0\n","    best_state = None\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        train_loss = train_epoch(train_loader, model, optimizer, criterion)\n","        roc_val, pr_val, mean_roc_val, mean_pr_val = evaluate(val_loader, model)\n","        scheduler.step()\n","\n","        print(f\"Epoch {epoch:03d} | loss={train_loss:.4f} | \"\n","              f\"val ROC={mean_roc_val:.4f} | val PR={mean_pr_val:.4f} | \"\n","              f\"LR={scheduler.get_last_lr()[0]:.2e}\")\n","\n","        if mean_roc_val > best_val_roc:\n","            best_val_roc = mean_roc_val\n","            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n","            print(\"  â†’ New best model!\")\n","\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","        model.to(device)\n","        torch.save(model.state_dict(), os.path.join(DATA_DIR, \"gine_textfusion_best.pt\"))\n","        print(\"Saved best model to gine_textfusion_best.pt\")\n","\n","    # Final test\n","    roc_test, pr_test, mean_roc_test, mean_pr_test = evaluate(test_loader, model)\n","    print(\"\\n=== FINAL TEST METRICS (GINE + text prompts + desc) ===\")\n","    for a in ASSAYS:\n","        print(f\"{a:13s} | ROC-AUC={roc_test[a]:.4f} | PR-AUC={pr_test[a]:.4f}\")\n","    print(\"---------------------------------------------\")\n","    print(f\"Mean            | ROC-AUC={mean_roc_test:.4f} | PR-AUC={mean_pr_test:.4f}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","source":["***PNA cross attention***"],"metadata":{"id":"FBxTUuxBsP20"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import PNAConv, global_mean_pool, global_max_pool\n","from torch_geometric.loader import DataLoader\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","\n","# ============================\n","# Data Loading\n","# ============================\n","\n","ASSAYS = [\n","    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n","    \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-gamma\",\n","    \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n","]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# ---- Load graphs ----\n","train_graphs = torch.load(\"graphs/train_2d.pt\")\n","val_graphs   = torch.load(\"graphs/val_2d.pt\")\n","test_graphs  = torch.load(\"graphs/test_2d.pt\")\n","\n","# ---- Load descriptors ----\n","use_desc = True\n","if use_desc:\n","    train_desc = np.load(r\"E:\\graphml project\\novel\\processed\\train_rdkit_desc.npz\")[\"X\"]\n","    val_desc   = np.load(r\"E:\\graphml project\\novel\\processed\\val_rdkit_desc.npz\")[\"X\"]\n","    test_desc  = np.load(r\"E:\\graphml project\\novel\\processed\\test_rdkit_desc.npz\")[\"X\"]\n","    desc_dim = train_desc.shape[1]\n","else:\n","    desc_dim = 32\n","    train_desc = np.zeros((len(train_graphs), desc_dim), dtype=np.float32)\n","    val_desc   = np.zeros((len(val_graphs), desc_dim), dtype=np.float32)\n","    test_desc  = np.zeros((len(test_graphs), desc_dim), dtype=np.float32)\n","\n","# ---- Attach features ----\n","def attach_features(graph_list, desc_array):\n","    for i, g in enumerate(graph_list):\n","        g.desc_features = torch.from_numpy(desc_array[i]).float()\n","    return graph_list\n","\n","train_graphs = attach_features(train_graphs, train_desc)\n","val_graphs   = attach_features(val_graphs, val_desc)\n","test_graphs  = attach_features(test_graphs, test_desc)\n","\n","BATCH_SIZE = 64\n","train_loader = DataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader   = DataLoader(val_graphs, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader  = DataLoader(test_graphs, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# ============================\n","# Text Prompts Definition\n","# ============================\n","\n","ASSAY_DESCRIPTIONS = {\n","    \"NR-AR\": \"androgen receptor binding and endocrine disruption potential\",\n","    \"NR-AR-LBD\": \"androgen receptor ligand binding domain interaction\",\n","    \"NR-AhR\": \"aryl hydrocarbon receptor activation and xenobiotic metabolism\",\n","    \"NR-Aromatase\": \"aromatase enzyme inhibition and steroid metabolism\",\n","    \"NR-ER\": \"estrogen receptor binding and hormonal activity\",\n","    \"NR-ER-LBD\": \"estrogen receptor ligand binding domain interaction\",\n","    \"NR-PPAR-gamma\": \"peroxisome proliferator-activated receptor gamma activation\",\n","    \"SR-ARE\": \"antioxidant response element activation and oxidative stress\",\n","    \"SR-ATAD5\": \"ATAD5 biomarker response and genotoxicity\",\n","    \"SR-HSE\": \"heat shock response element activation and protein stress\",\n","    \"SR-MMP\": \"mitochondrial membrane potential disruption and cytotoxicity\",\n","    \"SR-p53\": \"p53 tumor suppressor pathway activation and DNA damage response\"\n","}\n","\n","print(\"Assay Prompts:\")\n","for assay, desc in ASSAY_DESCRIPTIONS.items():\n","    print(f\"  {assay}: {desc}\")\n","\n","# ============================\n","# PNA + Text Enhanced Model\n","# ============================\n","\n","class PNAWithText(nn.Module):\n","    def __init__(self, node_feature_dim, edge_feature_dim, desc_feature_dim, n_tasks,\n","                 hidden_dim=128, num_layers=4, dropout=0.2):\n","        super().__init__()\n","\n","        self.n_tasks = n_tasks\n","        self.desc_feature_dim = desc_feature_dim\n","        self.hidden_dim = hidden_dim\n","\n","        # --- PNA Configuration ---\n","        aggregators = ['mean', 'min', 'max', 'std']\n","        scalers = ['identity', 'amplification', 'attenuation']\n","\n","        # Degree distribution for molecular graphs\n","        self.deg = torch.tensor([0, 1, 2, 3, 4])\n","\n","        # --- Feature Projections ---\n","        self.node_proj = nn.Sequential(\n","            nn.Linear(node_feature_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # --- PNA Layers ---\n","        self.convs = nn.ModuleList()\n","        self.batch_norms = nn.ModuleList()\n","\n","        for i in range(num_layers):\n","            conv = PNAConv(\n","                in_channels=hidden_dim,\n","                out_channels=hidden_dim,\n","                aggregators=aggregators,\n","                scalers=scalers,\n","                deg=self.deg,\n","                edge_dim=edge_feature_dim,\n","                towers=1,\n","                pre_layers=1,\n","                post_layers=1\n","            )\n","            self.convs.append(conv)\n","            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n","\n","        # --- Text Prompts ---\n","        self.assay_prompts = nn.Parameter(torch.randn(n_tasks, 128))\n","        self.text_proj = nn.Sequential(\n","            nn.Linear(128, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(0.1)\n","        )\n","\n","        # --- Cross-Attention Fusion ---\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=hidden_dim,\n","            num_heads=8,\n","            dropout=0.1,\n","            batch_first=True\n","        )\n","\n","        # --- Enhanced Classifier ---\n","        classifier_input_dim = hidden_dim * 2 + hidden_dim + desc_feature_dim\n","        self.classifier = nn.Sequential(\n","            nn.Linear(classifier_input_dim, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","            nn.Linear(512, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, n_tasks)\n","        )\n","\n","    def forward(self, data, assay_attention=None):\n","        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n","\n","        # 1. Project node features\n","        x = self.node_proj(x)\n","\n","        # 2. PNA Message Passing\n","        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):\n","            x_residual = x\n","            x = conv(x, edge_index, edge_attr)\n","            x = bn(x)\n","            x = F.elu(x)\n","\n","            # Residual connection\n","            if i % 2 == 1:\n","                x = x + x_residual\n","\n","            x = F.dropout(x, p=0.2, training=self.training)\n","\n","        # 3. Graph Readout\n","        mean_pool = global_mean_pool(x, batch)\n","        max_pool = global_max_pool(x, batch)\n","        graph_features = torch.cat([mean_pool, max_pool], dim=1)\n","\n","        # 4. Text Conditioning\n","        if assay_attention is None:\n","            assay_attention = torch.ones(len(graph_features), self.n_tasks,\n","                                       device=graph_features.device) / self.n_tasks\n","\n","        text_embeddings = torch.einsum('bi,ij->bj', assay_attention, self.assay_prompts)\n","        text_features = self.text_proj(text_embeddings)\n","\n","        # 5. Cross-Attention\n","        text_as_query = text_features.unsqueeze(1)\n","        graph_as_kv = graph_features[:, :self.hidden_dim].unsqueeze(1)\n","\n","        attended_features, _ = self.cross_attention(\n","            query=text_as_query,\n","            key=graph_as_kv,\n","            value=graph_as_kv\n","        )\n","        attended_features = attended_features.squeeze(1)\n","\n","        # 6. Descriptor Features\n","        desc_features = data.desc_features.view(len(graph_features), -1)\n","\n","        # 7. Final Fusion\n","        combined_features = torch.cat([graph_features, attended_features, desc_features], dim=1)\n","        logits = self.classifier(combined_features)\n","\n","        return logits\n","\n","# ============================\n","# Training Functions\n","# ============================\n","\n","def train_pna_epoch(loader, model, optimizer, criterion, device, n_tasks):\n","    model.train()\n","    total_loss = 0.0\n","    total_graphs = 0\n","\n","    for batch in loader:\n","        batch = batch.to(device)\n","        B = batch.num_graphs\n","\n","        # Smart assay attention\n","        y_batch = batch.y.float().view(B, n_tasks)\n","        w_batch = batch.weight.float().view(B, n_tasks)\n","        labeled_mask = (w_batch > 0).float()\n","\n","        if labeled_mask.sum() > 0:\n","            assay_attention = labeled_mask / labeled_mask.sum(dim=1, keepdim=True).clamp(min=1e-8)\n","        else:\n","            assay_attention = torch.ones(B, n_tasks, device=device) / n_tasks\n","\n","        logits = model(batch, assay_attention)\n","\n","        # Loss computation\n","        y = batch.y.float().view(-1, n_tasks)\n","        w = batch.weight.float().view(-1, n_tasks)\n","\n","        loss_unreduced = criterion(logits, y)\n","        mask = (w > 0).float()\n","        loss = (loss_unreduced * mask).sum() / mask.sum()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        total_loss += loss.item() * B\n","        total_graphs += B\n","\n","    return total_loss / total_graphs\n","\n","def evaluate(loader, model, device, assays):\n","    model.eval()\n","    all_probs = []\n","    all_labels = []\n","    all_weights = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch = batch.to(device)\n","            B = batch.num_graphs\n","\n","            assay_attention = torch.ones(B, len(assays), device=device) / len(assays)\n","\n","            logits = model(batch, assay_attention)\n","            probs = torch.sigmoid(logits)\n","\n","            y = batch.y.float().view(-1, len(assays))\n","            w = batch.weight.float().view(-1, len(assays))\n","\n","            all_probs.append(probs.cpu())\n","            all_labels.append(y.cpu())\n","            all_weights.append(w.cpu())\n","\n","    probs = torch.cat(all_probs, dim=0).numpy()\n","    labels = torch.cat(all_labels, dim=0).numpy()\n","    weights = torch.cat(all_weights, dim=0).numpy()\n","\n","    roc_scores = {}\n","    pr_scores = {}\n","\n","    for j, assay in enumerate(assays):\n","        mask = weights[:, j] > 0\n","        if mask.sum() < 5:\n","            roc_scores[assay] = np.nan\n","            pr_scores[assay] = np.nan\n","            continue\n","\n","        y_true = labels[mask, j]\n","        y_pred = probs[mask, j]\n","\n","        try:\n","            roc_scores[assay] = roc_auc_score(y_true, y_pred)\n","            pr_scores[assay] = average_precision_score(y_true, y_pred)\n","        except ValueError:\n","            roc_scores[assay] = np.nan\n","            pr_scores[assay] = np.nan\n","\n","    mean_roc = np.nanmean(list(roc_scores.values()))\n","    mean_pr = np.nanmean(list(pr_scores.values()))\n","    return roc_scores, pr_scores, mean_roc, mean_pr\n","\n","# ============================\n","# Main Execution\n","# ============================\n","\n","def main():\n","    EPOCHS = 100\n","    LR = 2e-4\n","    WEIGHT_DECAY = 1e-5\n","\n","    print(f\"Using device: {device}\")\n","\n","    # Initialize model\n","    sample = train_graphs[0]\n","    node_dim = sample.x.size(1)\n","    edge_dim = sample.edge_attr.size(1)\n","\n","    print(f\"Node features: {node_dim}\")\n","    print(f\"Edge features: {edge_dim}\")\n","    print(f\"Descriptor features: {desc_dim}\")\n","    print(f\"Number of tasks: {len(ASSAYS)}\")\n","\n","    model = PNAWithText(\n","        node_feature_dim=node_dim,\n","        edge_feature_dim=edge_dim,\n","        desc_feature_dim=desc_dim,\n","        n_tasks=len(ASSAYS),\n","        hidden_dim=128,\n","        num_layers=4\n","    ).to(device)\n","\n","    print(f\"PNA Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","    # Optimizer and loss\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=LR,\n","        weight_decay=WEIGHT_DECAY\n","    )\n","\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","    criterion = nn.BCEWithLogitsLoss(reduction='none')\n","\n","    # Training loop\n","    best_val_roc = -1.0\n","    print(\"\\nStarting PNA Training...\")\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        train_loss = train_pna_epoch(train_loader, model, optimizer, criterion, device, len(ASSAYS))\n","        roc_val, pr_val, mean_roc_val, mean_pr_val = evaluate(val_loader, model, device, ASSAYS)\n","        scheduler.step()\n","\n","        print(f\"Epoch {epoch:03d} | Loss: {train_loss:.4f} | Val ROC: {mean_roc_val:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n","\n","        if mean_roc_val > best_val_roc:\n","            best_val_roc = mean_roc_val\n","            torch.save(model.state_dict(), \"pna_text_best.pt\")\n","            print(f\"  â†’ New best! (ROC: {best_val_roc:.4f})\")\n","\n","    # Final test\n","    model.load_state_dict(torch.load(\"pna_text_best.pt\"))\n","    roc_test, pr_test, mean_roc_test, mean_pr_test = evaluate(test_loader, model, device, ASSAYS)\n","\n","    print(f\"\\n FINAL PNA TEST RESULTS:\")\n","    print(f\"ROC-AUC: {mean_roc_test:.4f}\")\n","    print(f\"PR-AUC: {mean_pr_test:.4f}\")\n","\n","    # Show per-assay results\n","    print(\"\\nPer-assay ROC-AUC:\")\n","    for assay in ASSAYS:\n","        print(f\"  {assay}: {roc_test[assay]:.4f}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"qjGOIIfbrtvi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Text enhanced weighted assay GINE MLP projection***"],"metadata":{"id":"huxuSJKvtEPL"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GINEConv, global_mean_pool, global_max_pool\n","from torch_geometric.loader import DataLoader\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","\n","# ============================\n","# Text-Enhanced Model WITHOUT ECFP\n","# ============================\n","\n","class TextEnhancedNoECFP(nn.Module):\n","    def __init__(self, node_feature_dim, edge_feature_dim, desc_feature_dim, n_tasks):\n","        super().__init__()\n","\n","        self.n_tasks = n_tasks\n","        self.desc_feature_dim = desc_feature_dim\n","\n","        # --- GNN Backbone ---\n","        nn1 = nn.Sequential(\n","            nn.Linear(node_feature_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128)\n","        )\n","        self.gnn_conv1 = GINEConv(nn1, edge_dim=edge_feature_dim)\n","\n","        nn2 = nn.Sequential(\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128)\n","        )\n","        self.gnn_conv2 = GINEConv(nn2, edge_dim=edge_feature_dim)\n","\n","        self.gnn_batch_norm1 = nn.BatchNorm1d(128)\n","        self.gnn_batch_norm2 = nn.BatchNorm1d(128)\n","\n","        # Graph output dimension\n","        gnn_out_dim = 256  # mean + max pool\n","\n","        # --- Learnable Text Prompts for Each Assay ---\n","        self.assay_prompts = nn.Parameter(torch.randn(n_tasks, 128))\n","\n","        # Text projection\n","        self.text_proj = nn.Sequential(\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.2)\n","        )\n","\n","        # --- Assay-Conditioned Fusion ---\n","        self.assay_weights = nn.Parameter(torch.ones(n_tasks, 2))  # [12, 2] for gnn, desc\n","\n","        # Final classifier (NO ECFP dimension)\n","        classifier_input_dim = 256 + desc_feature_dim + 128  # gnn + desc + text ONLY\n","        self.classifier = nn.Sequential(\n","            nn.Linear(classifier_input_dim, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","            nn.Linear(512, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, n_tasks)\n","        )\n","\n","    def forward_gnn(self, x, edge_index, edge_attr, batch):\n","        # GNN processing\n","        x = self.gnn_conv1(x, edge_index, edge_attr)\n","        x = self.gnn_batch_norm1(x)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=0.2, training=self.training)\n","\n","        x = self.gnn_conv2(x, edge_index, edge_attr)\n","        x = self.gnn_batch_norm2(x)\n","        x = F.elu(x)\n","\n","        # Readout\n","        mean_pool = global_mean_pool(x, batch)\n","        max_pool = global_max_pool(x, batch)\n","        return torch.cat([mean_pool, max_pool], dim=1)\n","\n","    def forward(self, data, assay_attention=None):\n","        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n","        B = data.num_graphs\n","\n","        # 1. Process GNN features\n","        graph_out = self.forward_gnn(x, edge_index, edge_attr, batch)  # [B, 256]\n","        desc_out = data.desc_features.view(B, self.desc_feature_dim)   # [B, desc_dim]\n","\n","        # 2. Text prompts for assay conditioning\n","        if assay_attention is None:\n","            # Default: equal attention to all assays\n","            text_weights = torch.ones(B, self.n_tasks, device=graph_out.device) / self.n_tasks\n","        else:\n","            text_weights = assay_attention\n","\n","        # Weighted average of text prompts\n","        text_feat = torch.einsum('bi,ij->bj', text_weights, self.assay_prompts)  # [B, 128]\n","        text_feat = self.text_proj(text_feat)  # [B, 128]\n","\n","        # 3. Assay-weighted fusion of modalities\n","        modality_weights = F.softmax(self.assay_weights, dim=1)  # [12, 2]\n","\n","        # Use average weights across assays\n","        avg_weights = modality_weights.mean(dim=0)  # [2]\n","\n","        # Apply weights to modalities (NO ECFP)\n","        weighted_graph = graph_out * avg_weights[0]\n","        weighted_desc = desc_out * avg_weights[1]\n","\n","        # 4. Concatenate all features (NO ECFP)\n","        combined = torch.cat([weighted_graph, weighted_desc, text_feat], dim=1)\n","\n","        # 5. Final prediction\n","        return self.classifier(combined)\n","\n","# ============================\n","# Text Prompts Definition\n","# ============================\n","\n","# Define meaningful text prompts for each assay\n","ASSAY_DESCRIPTIONS = {\n","    \"NR-AR\": \"androgen receptor binding and endocrine disruption potential\",\n","    \"NR-AR-LBD\": \"androgen receptor ligand binding domain interaction\",\n","    \"NR-AhR\": \"aryl hydrocarbon receptor activation and xenobiotic metabolism\",\n","    \"NR-Aromatase\": \"aromatase enzyme inhibition and steroid metabolism\",\n","    \"NR-ER\": \"estrogen receptor binding and hormonal activity\",\n","    \"NR-ER-LBD\": \"estrogen receptor ligand binding domain interaction\",\n","    \"NR-PPAR-gamma\": \"peroxisome proliferator-activated receptor gamma activation\",\n","    \"SR-ARE\": \"antioxidant response element activation and oxidative stress\",\n","    \"SR-ATAD5\": \"ATAD5 biomarker response and genotoxicity\",\n","    \"SR-HSE\": \"heat shock response element activation and protein stress\",\n","    \"SR-MMP\": \"mitochondrial membrane potential disruption\",\n","    \"SR-p53\": \"p53 tumor suppressor pathway activation and DNA damage\"\n","}\n","\n","# Convert to list in correct order\n","ASSAY_TEXTS = [ASSAY_DESCRIPTIONS[assay] for assay in ASSAYS]\n","\n","# ============================\n","# Training Components\n","# ============================\n","\n","def train_text_enhanced_epoch(loader, model, optimizer, criterion, device, n_tasks):\n","    model.train()\n","    total_loss = 0.0\n","    total_graphs = 0\n","\n","    for batch in loader:\n","        batch = batch.to(device)\n","        B = batch.num_graphs\n","\n","        # Strategy 1: Equal attention to all assays\n","        assay_attention = torch.ones(B, n_tasks, device=device) / n_tasks\n","\n","        # Strategy 2: Focus on assays with positive labels in this batch\n","        y_batch = batch.y.float().view(B, n_tasks)\n","        w_batch = batch.weight.float().view(B, n_tasks)\n","        labeled_mask = (w_batch > 0).float()\n","\n","        # If sample has specific assay labels, focus on those\n","        if labeled_mask.sum() > 0:\n","            assay_attention = labeled_mask / labeled_mask.sum(dim=1, keepdim=True).clamp(min=1e-8)\n","\n","        logits = model(batch, assay_attention)\n","\n","        # Targets and weights\n","        y = batch.y.float().view(-1, n_tasks)\n","        w = batch.weight.float().view(-1, n_tasks)\n","\n","        # Compute loss (only on labeled positions)\n","        loss_unreduced = criterion(logits, y)\n","        mask = (w > 0).float()\n","        loss = (loss_unreduced * mask).sum() / mask.sum()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        total_loss += loss.item() * B\n","        total_graphs += B\n","\n","    return total_loss / total_graphs\n","\n","def evaluate(loader, model, device, assays):\n","    model.eval()\n","    all_probs = []\n","    all_labels = []\n","    all_weights = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch = batch.to(device)\n","            B = batch.num_graphs\n","\n","            # For evaluation, use equal attention to all assays\n","            assay_attention = torch.ones(B, len(assays), device=device) / len(assays)\n","\n","            logits = model(batch, assay_attention)\n","            probs = torch.sigmoid(logits)\n","\n","            y = batch.y.float().view(-1, len(assays))\n","            w = batch.weight.float().view(-1, len(assays))\n","\n","            all_probs.append(probs.cpu())\n","            all_labels.append(y.cpu())\n","            all_weights.append(w.cpu())\n","\n","    probs = torch.cat(all_probs, dim=0).numpy()\n","    labels = torch.cat(all_labels, dim=0).numpy()\n","    weights = torch.cat(all_weights, dim=0).numpy()\n","\n","    roc_scores = {}\n","    pr_scores = {}\n","\n","    for j, assay in enumerate(assays):\n","        mask = weights[:, j] > 0\n","        if mask.sum() < 5:\n","            roc_scores[assay] = np.nan\n","            pr_scores[assay] = np.nan\n","            continue\n","\n","        y_true = labels[mask, j]\n","        y_pred = probs[mask, j]\n","\n","        try:\n","            roc_scores[assay] = roc_auc_score(y_true, y_pred)\n","            pr_scores[assay] = average_precision_score(y_true, y_pred)\n","        except ValueError:\n","            roc_scores[assay] = np.nan\n","            pr_scores[assay] = np.nan\n","\n","    mean_roc = np.nanmean(list(roc_scores.values()))\n","    mean_pr = np.nanmean(list(pr_scores.values()))\n","    return roc_scores, pr_scores, mean_roc, mean_pr\n","\n","# ============================\n","# Data Preparation (NO ECFP)\n","# ============================\n","\n","# Remove ECFP from your data loading\n","print(\"Preparing data WITHOUT ECFP...\")\n","train_graphs = torch.load(\"graphs/train_2d.pt\")\n","val_graphs   = torch.load(\"graphs/val_2d.pt\")\n","test_graphs  = torch.load(\"graphs/test_2d.pt\")\n","# Create zero ECFP features (minimal dimension to avoid errors)\n","train_fp = np.zeros((len(train_graphs), 1), dtype=np.float32)\n","val_fp = np.zeros((len(val_graphs), 1), dtype=np.float32)\n","test_fp = np.zeros((len(test_graphs), 1), dtype=np.float32)\n","fp_dim = 1\n","\n","# Keep descriptors\n","if use_desc:\n","    train_desc = np.load(r\"E:\\graphml project\\novel\\processed\\train_rdkit_desc.npz\")[\"X\"]\n","    val_desc = np.load(r\"E:\\graphml project\\novel\\processed\\val_rdkit_desc.npz\")[\"X\"]\n","    test_desc = np.load(r\"E:\\graphml project\\novel\\processed\\test_rdkit_desc.npz\")[\"X\"]\n","    desc_dim = train_desc.shape[1]\n","else:\n","    desc_dim = 32\n","    train_desc = np.zeros((len(train_graphs), desc_dim), dtype=np.float32)\n","    val_desc = np.zeros((len(val_graphs), desc_dim), dtype=np.float32)\n","    test_desc = np.zeros((len(test_graphs), desc_dim), dtype=np.float32)\n","\n","# Attach features (ECFP will be zeros)\n","def attach_features_no_ecfp(graph_list, desc_array):\n","    for i, g in enumerate(graph_list):\n","        g.fp_features = torch.zeros(1).float()  # Minimal ECFP\n","        g.desc_features = torch.from_numpy(desc_array[i]).float()\n","    return graph_list\n","\n","train_graphs = attach_features_no_ecfp(train_graphs, train_desc)\n","val_graphs = attach_features_no_ecfp(val_graphs, val_desc)\n","test_graphs = attach_features_no_ecfp(test_graphs, test_desc)\n","\n","# DataLoaders (same as before)\n","BATCH_SIZE = 64\n","train_loader = DataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_graphs, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# ============================\n","# Main Training Pipeline\n","# ============================\n","\n","def main():\n","    EPOCHS = 100\n","    LR = 2e-5\n","    WEIGHT_DECAY = 1e-5\n","\n","    print(\"Initializing Text-Enhanced GNN WITHOUT ECFP...\")\n","\n","    # Get dimensions\n","    sample = train_graphs[0]\n","    node_dim = sample.x.size(1)\n","    edge_dim = sample.edge_attr.size(1)\n","\n","    print(f\"Node features: {node_dim}\")\n","    print(f\"Edge features: {edge_dim}\")\n","    print(f\"Descriptor features: {desc_dim}\")\n","    print(f\"Number of tasks: {len(ASSAYS)}\")\n","    print(\"\\nUsing Assay Prompts:\")\n","    for assay, desc in ASSAY_DESCRIPTIONS.items():\n","        print(f\"  {assay}: {desc}\")\n","\n","    # Initialize model\n","    model = TextEnhancedNoECFP(\n","        node_feature_dim=node_dim,\n","        edge_feature_dim=edge_dim,\n","        desc_feature_dim=desc_dim,\n","        n_tasks=len(ASSAYS)\n","    ).to(device)\n","\n","    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","    # Optimizer and loss\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=LR,\n","        weight_decay=WEIGHT_DECAY\n","    )\n","\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode='max', factor=0.5, patience=10, verbose=True\n","    )\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction='none')\n","\n","    # Training loop\n","    best_val_roc = -1.0\n","    best_state = None\n","    patience = 15\n","    patience_counter = 0\n","\n","    print(\"\\nStarting Training...\")\n","    print(\"Epoch | Train Loss | Val ROC-AUC | Val PR-AUC | LR\")\n","    print(\"-\" * 55)\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        # Training\n","        train_loss = train_text_enhanced_epoch(\n","            train_loader, model, optimizer, criterion, device, len(ASSAYS)\n","        )\n","\n","        # Validation\n","        roc_val, pr_val, mean_roc_val, mean_pr_val = evaluate(val_loader, model, device, ASSAYS)\n","\n","        # Update learning rate\n","        scheduler.step(mean_roc_val)\n","\n","        print(f\"{epoch:5d} | {train_loss:.4f}      | {mean_roc_val:.4f}      | {mean_pr_val:.4f}    | {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","        # Save best model\n","        if mean_roc_val > best_val_roc:\n","            best_val_roc = mean_roc_val\n","            best_state = model.state_dict().copy()\n","            patience_counter = 0\n","            torch.save(model.state_dict(), \"text_enhanced_no_ecfp_best.pt\")\n","            print(f\"  â†’ New best! (ROC-AUC: {best_val_roc:.4f})\")\n","        else:\n","            patience_counter += 1\n","\n","        if patience_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch}\")\n","            break\n","\n","    print(f\"\\nTraining completed. Best validation ROC-AUC: {best_val_roc:.4f}\")\n","\n","    # Load best model for testing\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","        print(\"Loaded best model for testing\")\n","\n","    # Final evaluation\n","    roc_test, pr_test, mean_roc_test, mean_pr_test = evaluate(test_loader, model, device, ASSAYS)\n","\n","    print(\"\\n\" + \"=\" * 65)\n","    print(\"FINAL TEST METRICS (Text-Enhanced GNN WITHOUT ECFP)\")\n","    print(\"=\" * 65)\n","    for assay in ASSAYS:\n","        print(f\"{assay:15s} | ROC-AUC: {roc_test[assay]:.4f} | PR-AUC: {pr_test[assay]:.4f}\")\n","    print(\"-\" * 65)\n","    print(f\"{'Mean':15s} | ROC-AUC: {mean_roc_test:.4f} | PR-AUC: {mean_pr_test:.4f}\")\n","\n","    # Save final model\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'assay_prompts': model.assay_prompts.detach().cpu(),\n","        'assay_descriptions': ASSAY_DESCRIPTIONS,\n","        'test_metrics': {\n","            'roc_auc': roc_test,\n","            'pr_auc': pr_test,\n","            'mean_roc': mean_roc_test,\n","            'mean_pr': mean_pr_test\n","        },\n","        'config': {\n","            'use_ecfp': False,\n","            'use_text_prompts': True,\n","            'use_descriptors': True\n","        }\n","    }, \"text_enhanced_no_ecfp_final.pt\")\n","\n","    print(\"\\nModel saved as 'text_enhanced_no_ecfp_final.pt'\")\n","\n","    # Show learned prompt similarities\n","    print(\"\\nLearned assay prompt similarities:\")\n","    prompts = model.assay_prompts.detach().cpu()\n","    similarities = F.cosine_similarity(prompts.unsqueeze(1), prompts.unsqueeze(0), dim=2)\n","\n","    # Show top similar assay pairs\n","    similar_pairs = []\n","    for i in range(len(ASSAYS)):\n","        for j in range(i + 1, len(ASSAYS)):\n","            similar_pairs.append((i, j, similarities[i, j].item()))\n","\n","    similar_pairs.sort(key=lambda x: x[2], reverse=True)\n","    for i, j, sim in similar_pairs[:5]:  # Top 5 most similar\n","        print(f\"  {ASSAYS[i]:15s} â†” {ASSAYS[j]:15s}: {sim:.3f}\")\n","\n","# Run the training\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"zVHnbwZmskFy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Graph trasnformer with positional encoding Text+Graph***"],"metadata":{"id":"j5LwKVHQt8B6"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import TransformerConv, LayerNorm\n","from torch_geometric.utils import to_dense_batch, degree\n","from torch_geometric.data import DataLoader\n","import numpy as np\n","from sklearn.metrics import roc_auc_score\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# ============================\n","# CONFIGURATION\n","# ============================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\" Using device: {device}\")\n","\n","HIDDEN_DIM = 128\n","NUM_HEADS = 4\n","NUM_LAYERS = 4\n","DROPOUT = 0.4\n","BATCH_SIZE = 32\n","LR = 1e-4             # Slightly lower LR for stability with ASL\n","WEIGHT_DECAY = 1e-3\n","EPOCHS = 100\n","PATIENCE = 15\n","\n","ASSAYS = [\n","    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n","    \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-gamma\",\n","    \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n","]\n","N_TASKS = len(ASSAYS)\n","\n","# ============================\n","# 1. THE ENHANCED LOSS: ASYMMETRIC LOSS (ASL)\n","# ============================\n","class AsymmetricLossOptimized(nn.Module):\n","    \"\"\"\n","    ASL: Focuses on Hard Negatives and Positives.\n","    - gamma_neg=4: Heavily suppresses easy negatives (95% of data).\n","    - gamma_pos=1: Lightly focuses on hard positives.\n","    - clip=0.05: Completely ignores negatives with p < 0.05 (Noise removal).\n","    \"\"\"\n","    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n","        super(AsymmetricLossOptimized, self).__init__()\n","        self.gamma_neg = gamma_neg\n","        self.gamma_pos = gamma_pos\n","        self.clip = clip\n","        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n","        self.eps = eps\n","\n","    def forward(self, x, y):\n","        # x: Logits, y: Targets (0 or 1)\n","\n","        # Calculate Probabilities\n","        x_sigmoid = torch.sigmoid(x)\n","        xs_pos = x_sigmoid\n","        xs_neg = 1 - x_sigmoid\n","\n","        # Asymmetric Clipping\n","        if self.clip is not None and self.clip > 0:\n","            xs_neg = (xs_neg + self.clip).clamp(max=1)\n","\n","        # Basic Cross Entropy Components\n","        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n","        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n","\n","        # Asymmetric Focusing\n","        loss = -1 * los_pos * (1 - xs_pos) ** self.gamma_pos - \\\n","               1 * los_neg * (1 - xs_neg) ** self.gamma_neg\n","\n","        return loss\n","\n","# ============================\n","# 2. MULTI-TASK LEARNABLE WRAPPER\n","# ============================\n","class MultiTaskLossWrapper(nn.Module):\n","    \"\"\"\n","    Learns a variance (sigma) for each task.\n","    Loss = Loss_task / (2 * sigma^2) + log(sigma)\n","    This allows the model to dynamically balance the 12 tasks during training.\n","    \"\"\"\n","    def __init__(self, num_tasks):\n","        super().__init__()\n","        self.num_tasks = num_tasks\n","        # Initialize log_vars to 0 (sigma = 1)\n","        self.log_vars = nn.Parameter(torch.zeros((num_tasks)))\n","\n","    def forward(self, losses):\n","        # losses should be shape [Batch, NumTasks] or [NumTasks]\n","\n","        # Precision = 1 / (2 * sigma^2)\n","        precision = torch.exp(-self.log_vars)\n","\n","        # Weighted Loss\n","        weighted_loss = torch.sum(precision.to(losses.device) * losses)\n","\n","        # Regularization term (prevents sigma from going to infinity)\n","        log_term = torch.sum(self.log_vars)\n","\n","        return weighted_loss + log_term\n","\n","# ============================\n","# 3. GRAPH TRANSFORMER MODEL\n","# ============================\n","class LaplacianPE(nn.Module):\n","    def __init__(self, k=8, hidden_dim=128):\n","        super().__init__()\n","        self.k = k\n","        self.embedding = nn.Linear(k, hidden_dim)\n","\n","    def forward(self, data):\n","        deg = degree(data.edge_index[0], data.num_nodes, dtype=torch.float)\n","        # Safe Inverse\n","        deg_inv = deg.pow(-1)\n","        deg_inv[deg == 0] = 0\n","\n","        pe_list = [\n","            deg.unsqueeze(1),\n","            deg.pow(2).unsqueeze(1),\n","            deg_inv.unsqueeze(1),\n","            torch.log(deg + 1).unsqueeze(1)\n","        ]\n","        pe = torch.cat(pe_list, dim=1)\n","        if pe.size(1) < self.k:\n","            pe = F.pad(pe, (0, self.k - pe.size(1)))\n","        else:\n","            pe = pe[:, :self.k]\n","        return self.embedding(pe)\n","\n","class DeepGraphTransformer(nn.Module):\n","    def __init__(self, node_dim, edge_dim, num_tasks,\n","                 hidden_dim=128, num_heads=4, num_layers=4, dropout=0.4):\n","        super().__init__()\n","\n","        self.node_emb = nn.Linear(node_dim, hidden_dim)\n","        self.edge_emb = nn.Linear(edge_dim, hidden_dim)\n","        self.pe_enc = LaplacianPE(k=8, hidden_dim=hidden_dim)\n","\n","        # Learnable Semantic Prompts\n","        self.task_prompts = nn.Parameter(torch.randn(num_tasks, hidden_dim))\n","\n","        self.layers = nn.ModuleList()\n","        self.norms = nn.ModuleList()\n","\n","        for _ in range(num_layers):\n","            self.layers.append(\n","                TransformerConv(hidden_dim, hidden_dim // num_heads, heads=num_heads,\n","                                dropout=dropout, edge_dim=hidden_dim)\n","            )\n","            self.norms.append(LayerNorm(hidden_dim))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Prompt Attention\n","        self.prompt_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=dropout)\n","        self.prompt_norm = nn.LayerNorm(hidden_dim)\n","\n","        self.classifier = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, data):\n","        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n","\n","        h = self.node_emb(x)\n","        edge_h = self.edge_emb(edge_attr)\n","        h = h + self.pe_enc(data)\n","\n","        for conv, norm in zip(self.layers, self.norms):\n","            h_in = h\n","            h = conv(h, edge_index, edge_attr=edge_h)\n","            h = norm(h, batch)\n","            h = F.relu(h)\n","            h = self.dropout(h)\n","            h = h + h_in\n","\n","        h_nodes, mask = to_dense_batch(h, batch)\n","\n","        # Prompts Query the Molecule\n","        prompts = self.task_prompts.unsqueeze(0).expand(h_nodes.size(0), -1, -1)\n","\n","        attn_out, _ = self.prompt_attn(prompts, h_nodes, h_nodes, key_padding_mask=~mask)\n","        h_out = self.prompt_norm(prompts + attn_out)\n","\n","        # Output: [Batch, 12, Dim] -> [Batch, 12]\n","        logits = self.classifier(h_out).squeeze(-1)\n","        return logits\n","\n","# ============================\n","# 4. DATA LOADING\n","# ============================\n","def load_data():\n","    print(\"Loading Data...\")\n","    train_graphs = torch.load(\"graphs/train_2d.pt\")\n","    val_graphs = torch.load(\"graphs/val_2d.pt\")\n","    test_graphs = torch.load(\"graphs/test_2d.pt\")\n","\n","    def prepare(graphs):\n","        processed = []\n","        for g in graphs:\n","            # Generate dummy edges if missing (Safety)\n","            if not hasattr(g, 'edge_attr') or g.edge_attr is None:\n","                 g.edge_attr = torch.ones((g.edge_index.size(1), 1), dtype=torch.float)\n","            elif g.edge_attr.dim() == 1:\n","                g.edge_attr = g.edge_attr.unsqueeze(1)\n","            processed.append(g)\n","        return processed\n","\n","    return prepare(train_graphs), prepare(val_graphs), prepare(test_graphs)\n","\n","# ============================\n","# 5. TRAINING LOOP (WITH ASL + TASK WEIGHTING)\n","# ============================\n","def calculate_pos_weights(loader):\n","    \"\"\"\n","    Calculates the exact imbalance ratio for each of the 12 tasks.\n","    If 'NR-ER' has 1 positive for 50 negatives, weight = 50.\n","    \"\"\"\n","    all_y = []\n","    for batch in loader:\n","        all_y.append(batch.y.view(batch.num_graphs, -1))\n","\n","    all_y = torch.cat(all_y, dim=0)\n","    weights = []\n","\n","    print(\"\\nCalculated Task Weights:\")\n","    for i in range(12):\n","        # Filter out missing labels (-1 or NaN)\n","        valid = (all_y[:, i] != -1) & (~torch.isnan(all_y[:, i]))\n","        pos = (all_y[valid, i] == 1).sum().item()\n","        neg = (all_y[valid, i] == 0).sum().item()\n","\n","        # Calculate Ratio\n","        if pos > 0:\n","            w = neg / pos\n","        else:\n","            w = 1.0\n","\n","        # Clip max weight to 30 to prevent explosion\n","        w = min(w, 30.0)\n","        weights.append(w)\n","        print(f\"  Task {i+1}: {w:.2f}\")\n","\n","    return torch.tensor(weights).to(device)\n","\n","def train_epoch(model, loader, optimizer, pos_weights):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in loader:\n","        batch = batch.to(device)\n","        optimizer.zero_grad()\n","\n","        logits = model(batch)\n","        y = batch.y.view(batch.num_graphs, -1).float()\n","\n","        # Create mask for valid labels\n","        mask = (~torch.isnan(y)) & (y != -1)\n","        y_clean = torch.nan_to_num(y, 0.0)\n","\n","        if mask.sum() == 0: continue\n","\n","        # BCE with Explicit Positive Weighting\n","        # This handles the imbalance mathematically, not dynamically\n","        loss_ele = F.binary_cross_entropy_with_logits(\n","            logits,\n","            y_clean,\n","            reduction='none',\n","            pos_weight=pos_weights # <--- THE FIX\n","        )\n","\n","        # Apply Mask\n","        loss = (loss_ele * mask).sum() / mask.sum().clamp(min=1)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader):\n","    model.eval()\n","    # 1. Initialize with consistent names\n","    y_true_list, y_pred_list, y_mask_list = [], [], []\n","\n","    with torch.no_grad():\n","        for b in loader:\n","            b = b.to(device)\n","            logits = model(b)\n","\n","            # 2. Append to the correct lists\n","            y_true_list.append(b.y.view(b.num_graphs, -1).cpu())\n","            y_pred_list.append(torch.sigmoid(logits).cpu())\n","\n","            mask = (~torch.isnan(b.y.view(b.num_graphs, -1))) & (b.y.view(b.num_graphs, -1) != -1)\n","            y_mask_list.append(mask.cpu())\n","\n","    # 3. Concatenate\n","    y_true = torch.cat(y_true_list, 0).numpy()\n","    y_pred = torch.cat(y_pred_list, 0).numpy()\n","    y_mask = torch.cat(y_mask_list, 0).numpy()\n","\n","    rocs = []\n","    for i in range(12):\n","        valid = y_mask[:, i].astype(bool)\n","        # Check for sufficient data\n","        if valid.sum() < 5 or len(np.unique(y_true[valid, i])) < 2:\n","            continue\n","\n","        rocs.append(roc_auc_score(y_true[valid, i], y_pred[valid, i]))\n","\n","    return np.mean(rocs) if rocs else 0.0\n","\n","def evaluate_per_prompt(model, loader):\n","    model.eval()\n","    # 1. Initialize with consistent names\n","    y_true_list, y_pred_list, y_mask_list = [], [], []\n","\n","    with torch.no_grad():\n","        for b in loader:\n","            b = b.to(device)\n","            logits = model(b)\n","\n","            # 2. Append to the correct lists\n","            y_true_list.append(b.y.view(b.num_graphs, -1).cpu())\n","            y_pred_list.append(torch.sigmoid(logits).cpu())\n","\n","            mask = (~torch.isnan(b.y.view(b.num_graphs, -1))) & (b.y.view(b.num_graphs, -1) != -1)\n","            y_mask_list.append(mask.cpu())\n","\n","    # 3. Concatenate\n","    y_true = torch.cat(y_true_list, 0).numpy()\n","    y_pred = torch.cat(y_pred_list, 0).numpy()\n","    y_mask = torch.cat(y_mask_list, 0).numpy()\n","\n","    print(f\"\\n{'TASK':<20} | {'AUC':<10}\")\n","    print(\"-\" * 35)\n","\n","    res = {}\n","    for i, a in enumerate(ASSAYS):\n","        v = y_mask[:, i].astype(bool)\n","        if v.sum() < 5 or len(np.unique(y_true[v, i])) < 2:\n","            continue\n","\n","        auc = roc_auc_score(y_true[v, i], y_pred[v, i])\n","        res[a] = auc\n","        print(f\"{a:<20} | {auc:.4f}\")\n","\n","    print(\"-\" * 35)\n","    print(f\"MEAN: {np.mean(list(res.values())):.4f}\")\n","\n","# ============================\n","# 6. MAIN\n","# ============================\n","def main():\n","    train_data, val_data, test_data = load_data()\n","\n","    train_loader = DataLoader(train_data, BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_data, BATCH_SIZE)\n","    test_loader = DataLoader(test_data, BATCH_SIZE)\n","\n","    # 1. Calculate Weights\n","    pos_weights = calculate_pos_weights(train_loader)\n","\n","    sample = train_data[0]\n","    # Init Model\n","    model = DeepGraphTransformer(\n","        sample.x.shape[1], sample.edge_attr.shape[1], N_TASKS,\n","        hidden_dim=HIDDEN_DIM, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, dropout=DROPOUT\n","    ).to(device)\n","\n","    # Standard Optimizer (No wrapper parameters)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5)\n","\n","    print(\"\\nStarting Stabilized Transformer Training...\")\n","    best_auc = 0\n","\n","    for epoch in range(1, EPOCHS+1):\n","        loss = train_epoch(model, train_loader, optimizer, pos_weights)\n","        val_auc = evaluate(model, val_loader)\n","        scheduler.step(val_auc)\n","\n","        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f}\")\n","\n","        if val_auc > best_auc:\n","            best_auc = val_auc\n","            torch.save(model.state_dict(), \"best_stable_transformer.pt\")\n","            print(f\"  >>> New Best! (AUC: {best_auc:.4f})\")\n","\n","    # Final Test\n","    print(\"\\nFinal Test Breakdown:\")\n","    model.load_state_dict(torch.load(\"best_stable_transformer.pt\"))\n","    evaluate_per_prompt(model, test_loader)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"BzsfoFyzuFWH"},"execution_count":null,"outputs":[]}]}